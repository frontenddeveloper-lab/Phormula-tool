# from flask import Blueprint, request, jsonify
# import jwt
# import os
# from sqlalchemy import create_engine, text
# from dotenv import load_dotenv
# from config import Config
# from calendar import month_name, month_abbr, monthrange
# from datetime import date, datetime, timedelta
# import pandas as pd
# import numpy as np
# from openai import OpenAI
# import json
# from concurrent.futures import ThreadPoolExecutor, as_completed
# from app.utils.formulas_utils import (
#     uk_sales,
#     uk_credits,
#     uk_profit,
#     sku_mask,
#     safe_num,
# )
# from app.utils.email_utils import send_live_bi_email , get_user_email_by_id, has_recent_bi_email, mark_bi_email_sent
# # -----------------------------------------------------------------------------
# # ENV / DB SETUP
# # -----------------------------------------------------------------------------

# load_dotenv()
# SECRET_KEY = Config.SECRET_KEY

# db_url = os.getenv("DATABASE_URL")           # historical / settlement-style
# db_url2 = os.getenv("DATABASE_AMAZON_URL")   # amazon orders (Order model)

# engine_hist = create_engine(db_url)
# engine_live = create_engine(db_url2)

# # ðŸ”¹ NEW: OpenAI client
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# oa_client = OpenAI(api_key=OPENAI_API_KEY)

# live_data_bi_bp = Blueprint("live_data_bi_bp", __name__)


# # -----------------------------------------------------------------------------
# # DATE HELPERS
# # -----------------------------------------------------------------------------



# # ENV / DB SETUP ke paas, jaha engine_hist / engine_live define hua hai
# def is_blank_str(x):
#     return x is None or (isinstance(x, str) and x.strip() == "")

# def fetch_sku_product_mapping(user_id: int) -> pd.DataFrame:
#     """
#     sku_{user_id}_data_table se sku_uk -> product_name mapping uthata hai.
#     Sirf VALID mappings return karega:
#       - sku_uk present ho
#       - product_name present ho (non-null, non-empty)
#     """
#     table_name = f"sku_{user_id}_data_table"

#     query = text(f"""
#         SELECT
#             sku_uk,
#             product_name
#         FROM {table_name}
#     """)

#     with engine_hist.connect() as conn:
#         df = pd.read_sql(query, conn)

#     # null / duplicate clean up
#     if df.empty:
#         return df

#     # âœ… sku must exist
#     df = df.dropna(subset=["sku_uk"])

#     # âœ… product_name must exist and must not be blank
#     df = df.dropna(subset=["product_name"])
#     df["product_name"] = df["product_name"].astype(str)
#     df = df[df["product_name"].str.strip() != ""]

#     # âœ… normalize sku column for joining
#     df = df.rename(columns={"sku_uk": "sku"})

#     # optional: sku as string (safer for joins if orders.sku is string)
#     df["sku"] = df["sku"].astype(str).str.strip()

#     # âœ… remove duplicates
#     df = df.drop_duplicates(subset=["sku"])

#     return df


# def get_mtd_and_prev_ranges(as_of=None, start_day=None, end_day=None):
#     """
#     Default:
#       - current: current month 1st -> today (MTD)
#       - previous: previous month 1st -> previous month LAST day (full month)

#     Agar start_day & end_day diye gaye hain (frontend date range se):
#       - current: current month [start_day, end_day] (clamped to month length & today)
#       - previous: previous month [start_day, end_day] (clamped to that month length)
#     """
#     # --- resolve today / as_of ---
#     if as_of is None:
#         today = date.today()
#     else:
#         if isinstance(as_of, str):
#             today = datetime.strptime(as_of, "%Y-%m-%d").date()
#         elif isinstance(as_of, date):
#             today = as_of
#         else:
#             today = date.today()

#     # prev month/year
#     if today.month == 1:
#         prev_month = 12
#         prev_year = today.year - 1
#     else:
#         prev_month = today.month - 1
#         prev_year = today.year

#     # ---------- custom day range ----------
#     if start_day and end_day:
#         sd = int(min(start_day, end_day))
#         ed = int(max(start_day, end_day))

#         # current month clamp
#         last_day_curr = monthrange(today.year, today.month)[1]
#         sd_curr = max(1, min(sd, last_day_curr))
#         # end ko month & today dono se clamp kar do (future days avoid)
#         ed_curr = max(1, min(ed, last_day_curr, today.day))

#         current_period_start = date(today.year, today.month, sd_curr)
#         current_period_end = date(today.year, today.month, ed_curr)

#         # previous month clamp
#         last_day_prev = monthrange(prev_year, prev_month)[1]
#         sd_prev = max(1, min(sd, last_day_prev))
#         ed_prev = max(1, min(ed, last_day_prev))

#         prev_month_start = date(prev_year, prev_month, sd_prev)
#         prev_month_end = date(prev_year, prev_month, ed_prev)

#     # ---------- default behaviour (no range) ----------
#     else:
#         current_period_start = date(today.year, today.month, 1)
#         current_period_end = today

#         prev_month_start = date(prev_year, prev_month, 1)
#         last_day_prev = monthrange(prev_year, prev_month)[1]

#         # âœ… Align previous end day to current MTD day (clamp to prev month length)
#         prev_end_day = min(today.day, last_day_prev)
#         prev_month_end = date(prev_year, prev_month, prev_end_day)

#     return {
#         "current": {
#             "start": current_period_start,
#             "end": current_period_end,
#         },
#         "previous": {
#             "start": prev_month_start,
#             "end": prev_month_end,
#         },
#         "meta": {
#             "today": today,
#             "current_month": today.month,
#             "current_year": today.year,
#             "previous_month": prev_month,
#             "previous_year": prev_year,
#         },
#     }


# def month_num_to_name(m):
#     try:
#         m_int = int(m)
#         return month_name[m_int].lower() if 1 <= m_int <= 12 else None
#     except Exception:
#         return None


# def construct_prev_table_name(user_id, country, month, year):
#     """
#     user_{user_id}_{country_lower}_{monthname}{year}_data
#     e.g. user_10_uk_october2025_data
#     """
#     month_str = month_num_to_name(month)
#     if not month_str:
#         raise ValueError("Invalid month")
#     return f"user_{user_id}_{country.lower()}_{month_str}{year}_data"


# # -----------------------------------------------------------------------------
# # METRIC COMPUTATION PER SKU (using formula_utils)
# # -----------------------------------------------------------------------------

# def compute_sku_metrics_from_df(df: pd.DataFrame) -> list:
#     """
#     Given a raw settlement-style DataFrame with columns like:

#       sku, quantity, product_sales, cost_of_unit_sold, etc.

#     compute per-SKU metrics:

#       quantity
#       net_sales
#       profit
#       asp
#       unit_wise_profitability
#       sales_mix
#       product_name
#     """
#     if df is None or df.empty:
#         return []

#     df = df.copy()

#     # Keep only valid SKUs (same rule as formula_utils)
#     if "sku" in df.columns:
#         df = df.loc[sku_mask(df)]

#     if df.empty:
#         return []

#     # ---- quantity per SKU ----
#     if "quantity" in df.columns:
#         qty_df = (
#             df.assign(quantity=safe_num(df["quantity"]))
#               .groupby("sku", as_index=False)["quantity"]
#               .sum()
#         )
#     else:
#         qty_df = pd.DataFrame(columns=["sku", "quantity"])

#     # ---- product_name per SKU (first non-null) ----
#     if "product_name" in df.columns:
#         name_df = (
#             df[["sku", "product_name"]]
#             .dropna(subset=["sku"])
#             .groupby("sku", as_index=False)
#             .first()
#         )
#     else:
#         name_df = pd.DataFrame(columns=["sku", "product_name"])

#     # ---- sales, credits, profit per SKU via formula_utils ----
#     _, sales_by, _ = uk_sales(df)
#     _, credits_by, _ = uk_credits(df)
#     _, profit_by, _ = uk_profit(df)

#     if not sales_by.empty:
#         sales_by = sales_by.rename(columns={"__metric__": "sales_metric"})
#     if not credits_by.empty:
#         credits_by = credits_by.rename(columns={"__metric__": "credits_metric"})
#     if not profit_by.empty:
#         profit_by = profit_by.rename(columns={"__metric__": "profit_metric"})

#     # ---- merge everything ----
#     metrics = (
#         qty_df.merge(name_df, on="sku", how="left")
#               .merge(
#                   sales_by[["sku", "sales_metric"]]
#                   if not sales_by.empty
#                   else pd.DataFrame(columns=["sku", "sales_metric"]),
#                   on="sku", how="left"
#               )
#               .merge(
#                   credits_by[["sku", "credits_metric"]]
#                   if not credits_by.empty
#                   else pd.DataFrame(columns=["sku", "credits_metric"]),
#                   on="sku", how="left"
#               )
#               .merge(
#                   profit_by[["sku", "profit_metric"]]
#                   if not profit_by.empty
#                   else pd.DataFrame(columns=["sku", "profit_metric"]),
#                   on="sku", how="left"
#               )
#     )

#     # ---- compute final fields ----
#     metrics["quantity"] = safe_num(metrics["quantity"])
#     metrics["sales_metric"] = safe_num(metrics.get("sales_metric", 0.0))
#     metrics["credits_metric"] = safe_num(metrics.get("credits_metric", 0.0))
#     metrics["profit_metric"] = safe_num(metrics.get("profit_metric", 0.0))

#     metrics["net_sales"] = metrics["sales_metric"] + metrics["credits_metric"]
#     metrics["profit"] = metrics["profit_metric"]

#     # asp & per-unit profitability
#     qty_nonzero = metrics["quantity"].replace(0, np.nan)
#     metrics["asp"] = metrics["net_sales"] / qty_nonzero
#     metrics["unit_wise_profitability"] = metrics["profit"] / qty_nonzero

#     # sales_mix (% of net_sales)
#     total_net_sales = float(metrics["net_sales"].sum())
#     if total_net_sales != 0:
#         metrics["sales_mix"] = (metrics["net_sales"] / total_net_sales) * 100.0
#     else:
#         metrics["sales_mix"] = 0.0

#     # final list of dicts expected by growth logic
#     out_cols = [
#         "sku",
#         "product_name",
#         "quantity",
#         "asp",
#         "profit",
#         "sales_mix",
#         "net_sales",
#         "unit_wise_profitability",
#     ]
#     return (
#         metrics[out_cols]
#         .replace({np.nan: None})
#         .to_dict(orient="records")
#     )


# # -----------------------------------------------------------------------------
# # FETCH DATA: PREVIOUS PERIOD (historical table) + CURRENT MTD (orders)
# # -----------------------------------------------------------------------------

# def fetch_previous_period_data(user_id, country, prev_start: date, prev_end: date):
#     """
#     Return:
#       sku_metrics: list of per-SKU metrics (for growth calc)
#       daily_series: list of {date, quantity, net_sales} for line chart
#     """
#     table_name = construct_prev_table_name(
#         user_id=user_id,
#         country=country,
#         month=prev_start.month,
#         year=prev_start.year,
#     )

#     # Debug: show constructed table name
#     print(f"[DEBUG] Previous Period Table: {table_name}")

#     # Safer approach:
#     #  - build a subquery that casts date_time to date_ts
#     #  - use NULLIF to avoid casting '0' / '' values
#     query = text(f"""
#         SELECT *
#         FROM (
#             SELECT
#                 *,
#                 NULLIF(NULLIF(date_time, '0'), '')::timestamp AS date_ts
#             FROM {table_name}
#         ) t
#         WHERE date_ts >= :start_date
#           AND date_ts < :end_date_plus_one
#     """)

#     params = {
#         "start_date": datetime.combine(prev_start, datetime.min.time()),
#         "end_date_plus_one": datetime.combine(prev_end + timedelta(days=1), datetime.min.time()),
#     }

#     # Debug: show SQL + params
#     print("[DEBUG] SQL for previous period:")
#     print(query)
#     print("[DEBUG] Params:", params)

#     with engine_hist.connect() as conn:
#         result = conn.execute(query, params)
#         rows = result.fetchall()

#         # Debug: row count
#         print(f"[DEBUG] Fetched {len(rows)} rows from {table_name}")

#         if len(rows) > 0:
#             # Show first 2 rows for sanity
#             print("[DEBUG] First rows:", rows[:2])

#         if not rows:
#             print("[DEBUG] No previous period data found.")
#             return [], []

#         df = pd.DataFrame(rows, columns=result.keys())

#     # Debug: DataFrame shape
#     print(f"[DEBUG] DataFrame shape: {df.shape}")
#     print(f"[DEBUG] DataFrame columns: {list(df.columns)}")

#     # ---- per-SKU metrics (existing behaviour) ----
#     sku_metrics = compute_sku_metrics_from_df(df)

#     # Debug: Number of SKU items returned
#     print(f"[DEBUG] SKU Metrics Count: {len(sku_metrics)}")

#     # ---- daily series for line chart (quantity + net_sales) ----
#     daily_series = []

#     # Prefer using the parsed timestamp column if present
#     date_col_for_series = None
#     if "date_ts" in df.columns:
#         date_col_for_series = "date_ts"
#     elif "date_time" in df.columns:
#         date_col_for_series = "date_time"

#     if date_col_for_series:
#         tmp = df.copy()
#         tmp["date_only"] = pd.to_datetime(tmp[date_col_for_series]).dt.date

#         # quantity per day (agar column hai)
#         if "quantity" in tmp.columns:
#             tmp["quantity"] = safe_num(tmp["quantity"])
#             daily_qty = (
#                 tmp.groupby("date_only", as_index=False)["quantity"]
#                    .sum()
#             )
#             qty_map = {
#                 d: float(q)
#                 for d, q in zip(daily_qty["date_only"], daily_qty["quantity"])
#             }
#         else:
#             qty_map = {}

#         # net_sales per day (yahan simple approx: product_sales ka sum)
#         if "product_sales" in tmp.columns:
#             tmp["product_sales"] = safe_num(tmp["product_sales"])
#             daily_sales = (
#                 tmp.groupby("date_only", as_index=False)["product_sales"]
#                    .sum()
#             )
#             sales_map = {
#                 d: float(v)
#                 for d, v in zip(daily_sales["date_only"], daily_sales["product_sales"])
#             }
#         else:
#             sales_map = {}

#         all_dates = sorted(set(qty_map.keys()) | set(sales_map.keys()))

#         for d in all_dates:
#             daily_series.append(
#                 {
#                     "date": d.isoformat(),
#                     "quantity": qty_map.get(d, 0.0) if qty_map else None,
#                     "net_sales": sales_map.get(d, 0.0) if sales_map else None,
#                 }
#             )

#         # Debug: line chart points
#         print(f"[DEBUG] Daily series points: {len(daily_series)}")

#     return sku_metrics, daily_series

# from datetime import date, datetime, timedelta
# import pandas as pd
# import numpy as np
# from sqlalchemy import text

# def fetch_current_mtd_data(user_id, country, curr_start: date, curr_end: date, is_global: bool):
#     """
#     Return:
#       sku_metrics: list of per-SKU metrics (manual aggregation, no formula utils)
#       daily_series: list of {date, quantity, net_sales} for line chart

#     Notes:
#       - country and is_global are not used (kept in signature to avoid breaking callers).
#       - Liveorders table renamed to liveorders.
#       - liveorders does NOT have product_name or country columns.
#       - product_name in output is derived from sku (fallback) and optionally overridden via SKU mapping.
#       - Uses profit and cogs directly (assumes cogs is TOTAL per row).
#       - net_sales = product_sales + promotional_rebates
#     """
#     table_name = "liveorders"

#     query = text(f"""
#         SELECT
#             sku,
#             quantity,
#             cogs,
#             product_sales,
#             promotional_rebates,
#             profit,
#             purchase_date,
#             order_status
#         FROM {table_name}
#         WHERE user_id = :user_id
#           AND purchase_date >= :start_date
#           AND purchase_date < :end_date_plus_one
#     """)

#     params = {
#         "user_id": user_id,
#         "start_date": datetime.combine(curr_start, datetime.min.time()),
#         "end_date_plus_one": datetime.combine(curr_end + timedelta(days=1), datetime.min.time()),
#     }

#     print("\n[DEBUG] CURRENT MTD QUERY")
#     print(query)
#     print("[DEBUG] Params:", params)

#     with engine_live.connect() as conn:
#         result = conn.execute(query, params)
#         rows = result.fetchall()
#         print(f"[DEBUG] Current MTD rows fetched (full filter): {len(rows)}")

#         if not rows:
#             print("[DEBUG] No current MTD data found in liveorders table.")

#             diag1 = conn.execute(text(f"""
#                 SELECT MIN(purchase_date), MAX(purchase_date), COUNT(*)
#                 FROM {table_name}
#                 WHERE user_id = :user_id
#             """), {"user_id": user_id}).fetchone()
#             print("[DEBUG] User-level date range in liveorders:", diag1)

#             diag2 = conn.execute(text(f"""
#                 SELECT DISTINCT order_status
#                 FROM {table_name}
#                 WHERE user_id = :user_id
#             """), {"user_id": user_id}).fetchall()
#             print("[DEBUG] Order statuses for this user in liveorders:", diag2)

#             return [], []

#         df = pd.DataFrame(rows, columns=result.keys())

#     # âœ… fallback "product_name" = sku (so downstream/UI doesn't break)
#     df["sku"] = df["sku"].astype(str).str.strip()
#     df["product_name"] = df["sku"]

#     # âœ… default assume "unmapped"
#     df["__has_mapping__"] = False

#     # ðŸ”¹ SKU mapping se product_name override + __has_mapping__ flag set
#     try:
#         sku_map_df = fetch_sku_product_mapping(user_id)  # only valid mappings
#         if not sku_map_df.empty:
#             print(f"[DEBUG] Merging SKU mapping for user_id={user_id}")

#             sku_map_df = sku_map_df.copy()
#             sku_map_df["sku"] = sku_map_df["sku"].astype(str).str.strip()

#             mapped_skus = set(sku_map_df["sku"].dropna())
#             df["__has_mapping__"] = df["sku"].isin(mapped_skus)

#             df = df.merge(
#                 sku_map_df,
#                 on="sku",
#                 how="left",
#                 suffixes=("", "_from_sku_table"),
#             )

#             if "product_name_from_sku_table" in df.columns:
#                 df["product_name"] = df["product_name_from_sku_table"].combine_first(df["product_name"])
#                 df.drop(columns=["product_name_from_sku_table"], inplace=True)
#         else:
#             print("[DEBUG] SKU mapping DF empty, using SKU as product_name.")
#     except Exception as e:
#         print("[WARN] Failed to fetch/merge SKU product mapping:", e)

#     # ----------------------------
#     # âœ… Manual prep (no formula utils)
#     # ----------------------------
#     df["quantity"] = safe_num(df.get("quantity", 0))
#     df["profit"] = safe_num(df.get("profit", 0))
#     df["cogs"] = safe_num(df.get("cogs", 0))  # assumed TOTAL COGS per row

#     df["product_sales"] = safe_num(df.get("product_sales", 0))
#     df["promotional_rebates"] = safe_num(df.get("promotional_rebates", 0))

#     # âœ… net_sales = product_sales + promotional_rebates
#     df["net_sales"] = df["product_sales"] + df["promotional_rebates"]

#     # ----------------------------
#     # ---- per-SKU metrics (manual but compatible with growth pipeline) ----
#     # ----------------------------
#     sku_agg = (
#         df.groupby("sku", as_index=False)
#           .agg(
#               product_name=("product_name", "first"),  # now sku fallback or mapped name
#               quantity=("quantity", "sum"),
#               net_sales=("net_sales", "sum"),
#               profit=("profit", "sum"),
#               cogs=("cogs", "sum"),
#               __has_mapping__=("__has_mapping__", "max"),
#           )
#     )

#     qty_nonzero = sku_agg["quantity"].replace(0, np.nan)

#     # Required by existing growth + AI logic
#     sku_agg["asp"] = (sku_agg["net_sales"] / qty_nonzero).fillna(0.0)
#     sku_agg["unit_wise_profitability"] = (sku_agg["profit"] / qty_nonzero).fillna(0.0)

#     total_net_sales = float(sku_agg["net_sales"].sum())
#     sku_agg["sales_mix"] = (sku_agg["net_sales"] / total_net_sales * 100.0) if total_net_sales else 0.0

#     # Your requested metric
#     sku_agg["profit_per_unit"] = sku_agg["unit_wise_profitability"]

#     sku_metrics = sku_agg.to_dict(orient="records")

#     # ----------------------------
#     # ---- daily series for line chart (quantity + net_sales) ----
#     # ----------------------------
#     daily_series = []
#     if "purchase_date" in df.columns:
#         tmp = df.copy()
#         tmp["date_only"] = pd.to_datetime(tmp["purchase_date"]).dt.date

#         daily_qty = tmp.groupby("date_only", as_index=False)["quantity"].sum()
#         qty_map = {d: float(q) for d, q in zip(daily_qty["date_only"], daily_qty["quantity"])}

#         daily_sales = tmp.groupby("date_only", as_index=False)["net_sales"].sum()
#         sales_map = {d: float(v) for d, v in zip(daily_sales["date_only"], daily_sales["net_sales"])}

#         for d in sorted(set(qty_map.keys()) | set(sales_map.keys())):
#             daily_series.append(
#                 {
#                     "date": d.isoformat(),
#                     "quantity": qty_map.get(d, 0.0),
#                     "net_sales": sales_map.get(d, 0.0),
#                 }
#             )

#     return sku_metrics, daily_series




# # -----------------------------------------------------------------------------
# # GROWTH METRIC CALCULATION (same formulas as Business Insights)
# # -----------------------------------------------------------------------------

# growth_field_mapping = {
#     "quantity": "Unit Growth (%)",
#     "asp": "ASP Growth (%)",
#     "net_sales": "Sales Growth (%)",
#     "sales_mix": "Sales Mix Change (%)",
#     "unit_wise_profitability": "Profit Per Unit (%)",
#     "profit": "CM1 Profit Impact (%)",
# }


# def categorize_growth(value):
#     if value is None:
#         return "No Data"
#     if value >= 5:
#         return "High Growth"
#     elif value > 0.5:
#         return "Low Growth"
#     elif value < -0.5:
#         return "Negative Growth"
#     else:
#         return "No Growth"


# def safe_float_local(val):
#     try:
#         if val is None:
#             return None
#         return float(val)
#     except (ValueError, TypeError):
#         return None

# def round_numeric_values(obj, ndigits=2):
#     """
#     Recursively walk any dict/list and:
#     - round floats to `ndigits`
#     - convert None / NaN to 0.0 (so UI blanks don't appear)
#     """

#     # âœ… None -> 0
#     if obj is None:
#         return 0.0

#     # âœ… NaN (python float) -> 0
#     if isinstance(obj, float) and np.isnan(obj):
#         return 0.0

#     # float -> round
#     if isinstance(obj, float):
#         return round(obj, ndigits)

#     # numpy floats -> round (NaN safe)
#     if isinstance(obj, (np.floating,)):
#         v = float(obj)
#         if np.isnan(v):
#             return 0.0
#         return round(v, ndigits)

#     # numpy ints -> normal int
#     if isinstance(obj, (np.integer,)):
#         return int(obj)

#     # dict -> recurse values
#     if isinstance(obj, dict):
#         return {k: round_numeric_values(v, ndigits) for k, v in obj.items()}

#     # list/tuple -> recurse items
#     if isinstance(obj, (list, tuple)):
#         return [round_numeric_values(v, ndigits) for v in obj]

#     # strings/bool/etc as-is
#     return obj


# def build_segment_total_row(prev_segment, curr_segment, key="sku", label="Total"):
#     """
#     prev_segment / curr_segment: subset of prev_data / curr_data
#     (sirf woh SKUs jo top_80 me hain, etc.)

#     Return: ek row jaisa calculate_growth deta hai, bas aggregated.
#     """
#     # ---- totals for quantity / net_sales / profit ----
#     prev_qty = prev_net = prev_prof = 0.0
#     curr_qty = curr_net = curr_prof = 0.0

#     # previous
#     for r in prev_segment:
#         q = safe_float_local(r.get("quantity"))
#         s = safe_float_local(r.get("net_sales"))
#         p = safe_float_local(r.get("profit"))
#         if q is not None: prev_qty += q
#         if s is not None: prev_net += s
#         if p is not None: prev_prof += p

#     # current
#     for r in curr_segment:
#         q = safe_float_local(r.get("quantity"))
#         s = safe_float_local(r.get("net_sales"))
#         p = safe_float_local(r.get("profit"))
#         if q is not None: curr_qty += q
#         if s is not None: curr_net += s
#         if p is not None: curr_prof += p

#     # ASP / unit profit indexes (portfolio level)
#     prev_asp = prev_net / prev_qty if prev_qty else None
#     curr_asp = curr_net / curr_qty if curr_qty else None

#     prev_up = prev_prof / prev_qty if prev_qty else None
#     curr_up = curr_prof / curr_qty if curr_qty else None

#     # sales mix (sum of sku-wise mix)
#     prev_mix = sum(
#         (safe_float_local(r.get("sales_mix")) or 0.0) for r in prev_segment
#     )
#     curr_mix = sum(
#         (safe_float_local(r.get("sales_mix")) or 0.0) for r in curr_segment
#     )

#     # pseudo SKU rows
#     seg_id = f"{label.upper().replace(' ', '_')}_SEGMENT"

#     prev_row = {
#         key: seg_id,
#         "product_name": label,
#         "quantity": prev_qty,
#         "net_sales": prev_net,
#         "profit": prev_prof,
#         "asp": prev_asp,
#         "unit_wise_profitability": prev_up,
#         "sales_mix": prev_mix,
#     }

#     curr_row = {
#         key: seg_id,
#         "product_name": label,
#         "quantity": curr_qty,
#         "net_sales": curr_net,
#         "profit": curr_prof,
#         "asp": curr_asp,
#         "unit_wise_profitability": curr_up,
#         "sales_mix": curr_mix,
#     }

#     # existing logic reuse
#     seg_growth_list = calculate_growth([prev_row], [curr_row], key=key)
#     return seg_growth_list[0] if seg_growth_list else None


# def calculate_growth(prev_data, curr_data, key="sku", numeric_fields=None) -> list:
#     """
#     prev_data / curr_data: list[dict] with keys
#       quantity, asp, net_sales, sales_mix, unit_wise_profitability, profit, product_name, sku

#     Returns, per SKU:
#       - <field>_prev  (previous period raw value)
#       - <field>_curr  (current MTD raw value)
#       - % growth mapped via growth_field_mapping
#       - Sales Mix (Current)  (for categorization / frontend)
#       - new_or_reviving flag
#     """
#     if numeric_fields is None:
#         numeric_fields = list(growth_field_mapping.keys())

#     prev_dict = {row.get(key): row for row in prev_data if row.get(key)}
#     results = []

#     for row2 in curr_data:
#         item_key = row2.get(key)
#         if not item_key:
#             continue

#         growth_row = {
#             "product_name": row2.get("product_name"),
#             key: item_key,
#         }

#         # ---- Month2 / current sales mix ----
#         sales_mix_curr = safe_float_local(row2.get("sales_mix"))
#         growth_row["Sales Mix (Current)"] = round(sales_mix_curr, 2) if sales_mix_curr is not None else 0.0

#         # -----------------------------
#         # Case 1: Existing SKU (in prev_data)
#         # -----------------------------
#         if item_key in prev_dict:
#             row1 = prev_dict[item_key]

#             for field in numeric_fields:
#                 val1 = safe_float_local(row1.get(field))  # previous period
#                 val2 = safe_float_local(row2.get(field))  # current period

#                 # âœ… raw values for Excel/UI (no blanks)
#                 growth_row[f"{field}_prev"] = 0.0 if val1 is None else val1
#                 growth_row[f"{field}_curr"] = 0.0 if val2 is None else val2

#                 # âœ… % growth calculation (no blanks)
#                 if val1 is None or val2 is None:
#                     growth = 0.0
#                 elif val1 != 0:
#                     growth = round(((val2 - val1) / val1) * 100.0, 2)
#                 else:
#                     # prev = 0 => undefined, but as per requirement send 0
#                     growth = 0.0

#                 label = growth_field_mapping[field]  # e.g. "Unit Growth (%)"
#                 growth_row[label] = {
#                     "category": categorize_growth(growth),
#                     "value": growth,
#                 }

#         # -----------------------------
#         # Case 2: New / Reviving SKU
#         # -----------------------------
#         else:
#             growth_row["new_or_reviving"] = True

#             # prev row agar mil jaye (edge case: sku new_or_reviving mark hua, but prev_dict me row present ho sakta)
#             row1 = prev_dict.get(item_key, {})  # {} => prev missing

#             for field in numeric_fields:
#                 val1 = safe_float_local(row1.get(field))  # previous
#                 val2 = safe_float_local(row2.get(field))  # current

#                 # raw values
#                 growth_row[f"{field}_prev"] = 0.0 if val1 is None else val1
#                 growth_row[f"{field}_curr"] = 0.0 if val2 is None else val2

#                 label = growth_field_mapping[field]

#                 # âœ… RULE:
#                 # if prev > 0 => calculate growth %
#                 # if prev == 0 (or missing) => don't calculate, keep as "No Data"
#                 if val1 is not None and val1 > 0 and val2 is not None:
#                     growth = round(((val2 - val1) / val1) * 100.0, 2)
#                     growth_row[label] = {
#                         "category": categorize_growth(growth),
#                         "value": growth,
#                     }
#                 else:
#                     # prev is 0 / missing => keep "as-is" behavior
#                     growth_row[label] = {
#                         "category": "No Data",
#                         "value": 0.0,
#                     }


#         results.append(growth_row)

#     return results


# #-----------------------------------------------------------------------------
# # AI SUMMARY (overall header) â€“ now via ChatGPT with numeric fallback
# #-----------------------------------------------------------------------------
# summary_numeric_fields = [
#     "quantity",
#     "net_sales",
#     "profit",
#     "asp",
#     "unit_wise_profitability"
# ]


# def aggregate_totals(rows, fields=None):
#     """
#     Sum key numeric fields across a list of SKU dicts.
#     Used to get total units / sales / profit for each period.
#     """
#     if fields is None:
#         fields = summary_numeric_fields

#     totals = {f: 0.0 for f in fields}
#     for row in rows:
#         for f in fields:
#             v = safe_float_local(row.get(f))
#             if v is not None:
#                 totals[f] += v
#     return totals


# def pct_change(prev, curr):
#     if prev is None or prev == 0:
#         return None
#     return round(((curr - prev) / prev) * 100.0, 1)


# def describe_movement(pct):
#     """
#     Turn a % into a human phrase like 'up 4.2%' / 'strongly down 18.3%' / 'roughly flat'
#     """
#     if pct is None:
#         return "roughly flat"

#     abs_v = abs(pct)
#     if abs_v < 1:
#         return "roughly flat"

#     direction = "up" if pct > 0 else "down"
#     if abs_v < 5:
#         intensity = ""
#     elif abs_v < 15:
#         intensity = "moderately "
#     else:
#         intensity = "strongly "

#     return f"{intensity}{direction} {abs_v:.1f}%"


# def _build_rule_based_summary(prev_totals, curr_totals, top_80_skus, new_reviving,
#                               prev_label, curr_label):
#     """
#     Old numeric summary logic (fallback when AI fails).
#     """
#     qty_change = pct_change(prev_totals.get("quantity"),   curr_totals.get("quantity"))
#     sales_change = pct_change(prev_totals.get("net_sales"), curr_totals.get("net_sales"))
#     profit_change = pct_change(prev_totals.get("profit"),   curr_totals.get("profit"))

#     bullets = []

#     # 1) Overall movement
#     bullets.append(
#         f"{curr_label} vs {prev_label}: units are {describe_movement(qty_change)}, "
#         f"sales are {describe_movement(sales_change)}, and CM1 profit is {describe_movement(profit_change)}."
#     )

#     # 2) Concentration in top SKUs
#     bullets.append(
#         f"{len(top_80_skus)} SKUs account for roughly 80% of current sales."
#     )

#     # 3) New / reviving SKUs
#     if new_reviving:
#         bullets.append(
#             f"{len(new_reviving)} new or reviving SKUs are contributing incremental volume."
#         )
#     else:
#         bullets.append(
#             "No material contribution from new or reviving SKUs this period."
#         )

#     # keep between 2 and 4 bullet points
#     return bullets[:4]

# def build_sku_context(sku_rows, max_items=5):
#     """
#     Poore portfolio (ya jo bhi SKU list tum pass karo) ko kuch logical buckets
#     me todta hai taaki AI summary/action bullets specific products ka naam
#     le sake.

#     Ye function AI ko product-focused labels deta hai â€” sirf product_name.
#     """

#     # ------------------------
#     # IMPORTANT: Only product name as label
#     # ------------------------
#     def make_label(row):
#         name = (row.get("product_name") or "").strip()
#         sku = (row.get("sku") or "").strip()

#         if name:
#             return name   # âœ… product name first
#         # fallback if product_name missing
#         return "Unnamed SKU"

#     fast_growing_profitable = []
#     declining_high_mix = []
#     flat_but_large = []

#     def get_pct(row, key):
#         obj = row.get(key) or {}
#         return safe_float_local(obj.get("value"))

#     # ------------------------
#     # Build list of items with metrics
#     # ------------------------
#     for row in sku_rows:
#         mix = safe_float_local(row.get("Sales Mix (Current)"))
#         if mix is None:
#             continue

#         unit_g = get_pct(row, "Unit Growth (%)")
#         asp_g = get_pct(row, "ASP Growth (%)")
#         sales_g = get_pct(row, "Sales Growth (%)")
#         mix_g = get_pct(row, "Sales Mix Change (%)")
#         up_g   = get_pct(row, "Profit Per Unit (%)")
#         prof_g = get_pct(row, "CM1 Profit Impact (%)")

#         item = {
#             "label": make_label(row),            # âœ… AI ab product name se refer karega
#             "sku": row.get("sku"),
#             "product_name": row.get("product_name"),

#             # current mix
#             "sales_mix_curr": mix,

#             # % growth
#             "unit_growth_pct": unit_g,
#             "asp_growth_pct": asp_g,
#             "sales_growth_pct": sales_g,
#             "mix_change_pct": mix_g,
#             "unit_profit_pct": up_g,
#             "profit_growth_pct": prof_g,

#             # raw previous/current values
#             "quantity_prev": safe_float_local(row.get("quantity_prev")),
#             "quantity_curr": safe_float_local(row.get("quantity_curr")),
#             "asp_prev": safe_float_local(row.get("asp_prev")),
#             "asp_curr": safe_float_local(row.get("asp_curr")),
#             "net_sales_prev": safe_float_local(row.get("net_sales_prev")),
#             "net_sales_curr": safe_float_local(row.get("net_sales_curr")),
#             "sales_mix_prev": safe_float_local(row.get("sales_mix_prev")),
#             "sales_mix_curr_raw": safe_float_local(row.get("sales_mix_curr")),
#             "unit_profit_prev": safe_float_local(row.get("unit_wise_profitability_prev")),
#             "unit_profit_curr": safe_float_local(row.get("unit_wise_profitability_curr")),
#             "profit_prev": safe_float_local(row.get("profit_prev")),
#             "profit_curr": safe_float_local(row.get("profit_curr")),
#         }

#         # ------------------------
#         # Categorization
#         # ------------------------
#         if (
#             sales_g is not None
#             and prof_g is not None
#             and sales_g > 5
#             and prof_g > 5
#             and mix >= 2
#         ):
#             fast_growing_profitable.append(item)

#         elif sales_g is not None and sales_g < -5 and mix >= 2:
#             declining_high_mix.append(item)

#         elif mix >= 5 and (sales_g is None or abs(sales_g) <= 2):
#             flat_but_large.append(item)

#     # ------------------------
#     # Sort & trim
#     # ------------------------
#     fast_growing_profitable = sorted(
#         fast_growing_profitable,
#         key=lambda x: x["sales_growth_pct"] or 0,
#         reverse=True,
#     )[:max_items]

#     declining_high_mix = sorted(
#         declining_high_mix,
#         key=lambda x: x["sales_growth_pct"] or 0,
#     )[:max_items]

#     flat_but_large = sorted(
#         flat_but_large,
#         key=lambda x: x["sales_mix_curr"] or 0,
#         reverse=True,
#     )[:max_items]

#     return {
#         "fast_growing_profitable": fast_growing_profitable,
#         "declining_high_mix": declining_high_mix,
#         "flat_but_large": flat_but_large,
#     }


# def build_ai_summary(
#     prev_totals,
#     curr_totals,
#     top_80_skus,
#     new_reviving,
#     prev_label,
#     curr_label,
#     sku_context=None,
# ):
#     def safe0(x):
#         v = safe_float_local(x)
#         return v if v is not None else 0.0

#     # Overall numeric values
#     qty_prev = safe0(prev_totals.get("quantity"))
#     qty_curr = safe0(curr_totals.get("quantity"))

#     sales_prev = safe0(prev_totals.get("net_sales"))
#     sales_curr = safe0(curr_totals.get("net_sales"))

#     prof_prev = safe0(prev_totals.get("profit"))
#     prof_curr = safe0(curr_totals.get("profit"))

#     asp_prev_idx = safe0(prev_totals.get("asp"))
#     asp_curr_idx = safe0(curr_totals.get("asp"))

#     up_prev_idx = safe0(prev_totals.get("unit_wise_profitability"))
#     up_curr_idx = safe0(curr_totals.get("unit_wise_profitability"))

#     qty_pct = pct_change(qty_prev, qty_curr)
#     sales_pct = pct_change(sales_prev, sales_curr)
#     prof_pct = pct_change(prof_prev, prof_curr)
#     asp_pct = pct_change(asp_prev_idx, asp_curr_idx)
#     up_pct = pct_change(up_prev_idx, up_curr_idx)

#     if sku_context is None:
#         sku_context = {
#             "fast_growing_profitable": [],
#             "declining_high_mix": [],
#             "flat_but_large": [],
#         }

#     # âœ… IMPORTANT: include SKU rows so product_name is available to the model
#     payload = {
#         "periods": {
#             "previous": {
#                 "label": prev_label,
#                 "quantity_total": qty_prev,
#                 "net_sales_total": sales_prev,
#                 "profit_total": prof_prev,
#                 "asp_sum_index": asp_prev_idx,
#                 "unit_profit_sum_index": up_prev_idx,
#             },
#             "current": {
#                 "label": curr_label,
#                 "quantity_total": qty_curr,
#                 "net_sales_total": sales_curr,
#                 "profit_total": prof_curr,
#                 "asp_sum_index": asp_curr_idx,
#                 "unit_profit_sum_index": up_curr_idx,
#             },
#         },
#         "pct_changes": {
#             "quantity_pct": qty_pct,
#             "net_sales_pct": sales_pct,
#             "profit_pct": prof_pct,
#             "asp_index_pct": asp_pct,
#             "unit_profit_index_pct": up_pct,
#         },
#         "portfolio": {
#             "top_80_skus_count": len(top_80_skus),
#             "new_reviving_skus_count": len(new_reviving),
#         },
#         "sku_context": sku_context,
#         "sku_tables": {
#             "top_80_skus": top_80_skus,
#             "new_reviving_skus": new_reviving,
#         },
#     }

#     data_block = json.dumps(payload, indent=2)

#     prompt = f"""
# You are a senior ecommerce business analyst.

# You receive JSON containing:
# - Overall totals and % change for units, net sales, profit, ASP index and unit profit index
# - SKU tables in sku_tables.top_80_skus and sku_tables.new_reviving_skus including product_name and SKU-wise metrics

# GOAL
# Produce:
# 1) A short overall business summary (3â€“5 bullets)
# 2) Exactly 5 detailed SKU-wise recommendations in the EXACT format below.

# ====================
# SUMMARY (3â€“5 bullets)
# ====================
# Write 3â€“5 short bullets describing, in simple language:
# - How overall units, sales and profit moved (using quantity_pct, net_sales_pct, profit_pct)
# - Any big change in ASP or unit profit (asp_index_pct, unit_profit_index_pct)
# - Whether performance is coming more from volume, pricing, or a few big SKUs

# ====================
# ACTIONS (exactly 5)
# ====================

# Pick SKUs only from:
# - sku_tables.top_80_skus
# - sku_tables.new_reviving_skus

# Return EXACTLY 5 action bullets.

# âœ… EACH action bullet MUST follow this exact layout with line breaks:

# Line 1: "Product name - <product_name>"
# Line 2-3: One paragraph of exactly 2 sentences (metrics + causal chain + mix)
# Line 4: (blank line)
# Line 5: One action sentence ONLY

# So the action_bullet string must look like:
# Product name - Classic

# The increase in ASP by 13.27% resulted in a dip in units by 25.91%, which also resulted in sales falling by 16.08%. The sales mix is down by 15.93%, reducing its contribution, and profit is up by 10.74%.

# Reduce ASP slightly to improve traction.

# -----------------------------
# Metrics paragraph rules (Line 2-3)
# -----------------------------
# - Exactly 2 sentences.

# - Sentence 1: Use ONLY the SKU metrics for ASP, Units and Sales (where values exist).
#   Use the same flow as the original causal chain, but adjust tone only in this special case:
#   - Default tone:
#     "The increase/decrease in ASP by X% resulted in a dip/growth in units by Y%, which also resulted in sales falling/increasing by Z%."
#   - If ASP, Units, and Sales are ALL negative (all down), do NOT imply ASP caused units. Use this co-movement tone instead:
#     "There is a decrease in ASP by X% and also a dip in units by Y%, which resulted in sales falling by Z%."

# - Sentence 2: Must mention Sales Mix Change (%) if available (up or down) in the same original style:
#   "The sales mix is up/down by M%, increasing/reducing its contribution."
#   If profitability metrics are available for that SKU, append ONLY ONE of them to the SAME sentence without breaking flow and without using "while/since/because":
#   Prefer in this order:
#   1) Profit (%)
#   2) Profit Per Unit (%)
#   Append as:
#   ", and profit is up/down by A%."
#   OR ", and profit per unit is up/down by B%."
#   (If profitability metric is not available, skip it.)

# - Do NOT invent numbers and do NOT add extra reasons.

# -----------------------------
# Allowed action sentences (Line 5 only)
# -----------------------------
# Use exactly ONE of these sentences, verbatim:
# - "Check ads and visibility campaigns for this product."
# - "Review the visibility setup for this product."
# - "Reduce ASP slightly to improve traction."
# - "Increase ASP slightly to strengthen margins."
# - "Maintain current ASP and monitor performance."
# - "Monitor performance closely for now."
# - "Check Amazon fees or taxes for this product as profit is down despite growth."

# Decision guidance:
# - If ASP is strongly up and units are down: prefer "Reduce ASP slightly to improve traction."
# - If units and sales are down and ASP is flat or slightly up: prefer visibility lines.
# - If profit/unit profit is very strong and units are stable/slightly down: prefer maintain/increase ASP.
# - If ASP, units, sales, and sales mix are up but profit is down: prefer "Check Amazon fees or taxes for this product as profit is down despite growth."

# Ignore:
# - Any row where product_name is "Total" or contains "Total".


# OUTPUT FORMAT
# Return ONLY valid JSON:
# {{
#   "summary_bullets": ["...", "..."],
#   "action_bullets": ["Action 1", "Action 2", "Action 3", "Action 4", "Action 5"]
# }}
# Do not add any extra keys. Do not wrap in Markdown.

# DATA
# {data_block}
# """

#     try:
#         ai_response = oa_client.chat.completions.create(
#             model="gpt-4o",
#             messages=[
#                 {
#                     "role": "system",
#                     "content": (
#                         "You are a senior ecommerce analyst. "
#                         "Return only valid JSON with summary_bullets and action_bullets. "
#                         "Each action_bullet must follow the exact 3-block layout: "
#                         "Product name line, blank line, 2-sentence metrics paragraph, blank line, one action line."
#                     ),
#                 },
#                 {"role": "user", "content": prompt},
#             ],
#             max_tokens=900,
#             temperature=0.2,
#             response_format={"type": "json_object"},
#         )

#         content = ai_response.choices[0].message.content.strip()
#         parsed = json.loads(content)

#         sum_bullets = parsed.get("summary_bullets", []) or []
#         act_bullets = parsed.get("action_bullets", []) or []

#         sum_clean = [str(b).strip() for b in sum_bullets if str(b).strip()]
#         act_clean = [str(b).strip() for b in act_bullets if str(b).strip()]

#         return {
#             "summary_bullets": sum_clean[:5],
#             "action_bullets": act_clean[:5],
#         }

#     except Exception as e:
#         print("[ERROR] AI summary generation failed, falling back:", e)

#     fallback_summary = _build_rule_based_summary(
#         prev_totals,
#         curr_totals,
#         top_80_skus,
#         new_reviving,
#         prev_label,
#         curr_label,
#     )

#     fallback_actions = [
#         "Product name - Key SKUs\n\nUnits and sales have softened this period. The sales mix has also weakened, reducing contribution.\n\nReview the visibility setup for this product.",
#         "Product name - Key SKUs\n\nPricing changes appear to be impacting volume and sales movement. The sales mix change indicates shifting contribution.\n\nMaintain current ASP and monitor performance.",
#         "Product name - Key SKUs\n\nSome high-mix SKUs show volume decline impacting sales outcomes. The sales mix has moved down, reducing contribution.\n\nCheck ads and visibility campaigns for this product.",
#         "Product name - Key SKUs\n\nProfitability is improving even where sales are softer across some SKUs. The sales mix change shows contribution movement.\n\nMonitor performance closely for now.",
#         "Product name - Key SKUs\n\nA few SKUs may need pricing support to improve traction. The sales mix indicates contribution shifts.\n\nReduce ASP slightly to improve traction.",
#     ]

#     return {
#         "summary_bullets": fallback_summary,
#         "action_bullets": fallback_actions[:5],
#     }


# #-----------------------------------------------------------------------------
# # ChatGPT insight generator for live MTD vs previous (per-SKU)
# #-----------------------------------------------------------------------------

# def generate_live_insight(item, country, is_global, prev_label, curr_label):
#     """
#     Generate AI insight for a single SKU row from live_mtd_vs_previous growth_data.
#     item: one dict from growth_data/top_80_skus/new_reviving/etc.
#     """
#     sku = item.get("sku")
#     product_name = (item.get("product_name") or "this product").strip()

#     # Decide key: for global without SKU, fall back to product_name
#     if is_global and not sku:
#         key = product_name
#     elif sku:
#         key = sku
#     else:
#         key = product_name

#     is_new_or_reviving = item.get("new_or_reviving", False)

#     # Data for the model (includes fields like quantity_prev/curr, net_sales_prev/curr, growth %, etc.)
#     data_block = json.dumps(item, indent=2)

#     if is_new_or_reviving:
#         # New / reviving SKU prompt
#         prompt = f"""
# You are a senior ecommerce business analyst. The following is a new or reviving product (no meaningful previous-period baseline).
# Compare it only within the current period and talk about its launch strength.

# Context:
# - Country: {country}
# - Previous period label: {prev_label}
# - Current period label: {curr_label}

# Details for '{product_name}'

# Observations:
# - List the 2â€“3 most important observations about this product's current performance
#   (units sold, ASP, net sales, profit per unit, etc.) using absolute values from the data.
# - Comment on launch/return momentumâ€”e.g., strong debut, moderate start, slow start.
# - Call out any potential red flags (e.g., high ASP but very low volume, low unit profitability, etc.).

# Improvements:
# - Suggest clear, concrete next actions for:
#   â€¢ Marketing
#   â€¢ Sales / Commercial
#   â€¢ Operations / Supply
# - Make each action specific and easy to execute.

# Sales Volume:
# â€¢ Comment on volume and what it says about early traction.
# â€¢ Suggest one commercial lever to improve or scale volume.

# ASP:
# â€¢ Comment on price positioning; suggest whether to test price up/down or hold.

# Profitability:
# â€¢ Comment on profit per unit or total profit; suggest if costs or pricing need optimization.

# End with one line:
# â€¢ Verdict: should this SKU be scaled quickly, tested more, or carefully repositioned? And why?

# Instructions:
# - Use plain text with bullet points only.
# - DO NOT use Markdown formatting (no **bold**, no headings).
# - Do NOT compare to previous periods (assume no baseline).
# - Use actual numbers or percentages from the data whenever they are present.

# Data:
# {data_block}
# """
#     else:
#         # Existing SKU with prev vs current
#         prompt = f"""
# You are a senior ecommerce business analyst. The data below shows a product's performance
# comparing a previous period vs the current MTD.

# Context:
# - Country: {country}
# - Previous period: {prev_label}
# - Current period: {curr_label}

# Details for '{product_name}'

# Observations:
# - List the 2â€“3 most important changes using ONLY the given metrics:
#   â€¢ quantity_prev vs quantity_curr
#   â€¢ net_sales_prev vs net_sales_curr
#   â€¢ profit_prev vs profit_curr
#   â€¢ asp_prev vs asp_curr
#   â€¢ unit_wise_profitability_prev vs unit_wise_profitability_curr
#   â€¢ and % fields like "Unit Growth (%)", "Sales Growth (%)", etc.
# - Use the exact causal tone wherever % values exist:
#   "The increase/decrease in ASP by X% resulted in a dip/growth in units by Y%, which also resulted in sales falling/increasing by Z%."
# - In at least one observation, mention Sales Mix Change (%) direction if present (up/down).
# - Do NOT add assumptions like stock issues, supply constraints, replenishment, OOS, or fulfillment problems.

# Improvements:
# - Provide exactly 3â€“5 action bullets.
# - Each action bullet MUST be exactly ONE sentence and MUST be chosen ONLY from the list below, verbatim (no edits):
#   â€¢ "Check ads and visibility campaigns for this product."
#   â€¢ "Review the visibility setup for this product."
#   â€¢ "Reduce ASP slightly to improve traction."
#   â€¢ "Increase ASP slightly to strengthen margins."
#   â€¢ "Monitor performance closely and reassess next steps."
#   â€¢ "Monitor performance closely for now."
# - Do NOT add any other recommendations, explanations, or extra words.
# - Do NOT mention stock, inventory, supply, operations, OOS, logistics, replenishment, or warehousing.
# - Decision guidance:
#   â€¢ If ASP is strongly up and units are down: prefer "Reduce ASP slightly to improve traction."
#   â€¢ If units and sales are down and ASP is flat or slightly up: prefer visibility lines.
#   â€¢ If profit/unit profit is very strong and units are stable/slightly down: prefer maintain/increase ASP.

# Then, for each metric, add:

# Unit Growth:
# â€¢ [Explain reasons for the growth/decline using ONLY available signals like unit trend vs ASP trend and what that implies about demand/visibility/conversion.]
# â€¢ [Choose ONE action bullet from the Improvements list that best fits the unit pattern and paste it verbatim.]

# ASP:
# â€¢ [Explain why ASP changed using ONLY available signals like pricing changes, discounting intensity, or product/pack/channel mix shifts (premium vs value) without referencing costs.]
# â€¢ [Choose ONE action bullet from the Improvements list that best fits the ASP direction and paste it verbatim.]

# Sales:
# â€¢ [Describe sales trend by explicitly tying it to Units Ã— ASP (e.g., â€œsales down mainly due to units decline while ASP was flat/upâ€ or â€œsales up driven by ASP lift with stable unitsâ€).]
# â€¢ [Choose ONE action bullet from the Improvements list that best fits the sales pattern and paste it verbatim.]

# Profit:
# â€¢ [Explain profit change using ONLY available signals like sales movement plus realized pricing/discounting/mix impact, and avoid any mention of COGS or cost changes.]
# â€¢ [Choose ONE action bullet from the Improvements list that best aligns with protecting/improving profitability given the observed trend and paste it verbatim.]

# Unit Profitability:
# â€¢ [Explain per-unit profit change using ONLY available signals like realized price/discounting and mix (higher-priced variants) impact, without mentioning COGS.]
# â€¢ [Choose ONE action bullet from the Improvements list that best fits per-unit profit strength/weakness and paste it verbatim.]



# Instructions:
# - Use plain text with bullets only.
# - DO NOT use Markdown formatting (no **bold**, no italics, no headers).
# - Avoid labels like "Root cause:" or "Action item:". Just use bullet points.
# - Use % values and trends from the data for every observation.
# - Make all insights easy for business teams to act on.

# Data:
# {data_block}
# """

#     try:
#         ai_response = oa_client.chat.completions.create(
#             model="gpt-4o",
#             messages=[
#                 {
#                     "role": "system",
#                     "content": (
#                         "You are a senior ecommerce analyst. "
#                         "Respond in plain text with bullet points only. "
#                         "Do not use Markdown formatting."
#                     ),
#                 },
#                 {"role": "user", "content": prompt},
#             ],
#             max_tokens=900,
#             temperature=0.4,
#         )

#         ai_text = ai_response.choices[0].message.content.strip()

#         # ===== DEBUG: print exactly what model returned =====
#         print("\n================ AI INSIGHT DEBUG ================")
#         print("KEY:", key, "| SKU:", sku, "| Product:", product_name, "| Global:", is_global, "| New/Reviving:", is_new_or_reviving)
#         print("INSIGHT (raw):\n", ai_text)
#         print("INSIGHT (repr):\n", repr(ai_text))   # shows \n, \t etc clearly
#         print("==================================================\n")

#         return key, {
#             "sku": sku,
#             "product_name": product_name,
#             "insight": ai_text,
#             "key_used": key,
#             "is_global": is_global,
#             "is_new_or_reviving": is_new_or_reviving,
#         }
#     except Exception as e:
#         # In case of API error, return a debug-friendly insight
#         return key, {
#             "sku": sku,
#             "product_name": product_name,
#             "insight": f"Error generating insight: {str(e)}",
#             "key_used": key,
#             "is_global": is_global,
#             "is_new_or_reviving": is_new_or_reviving,
#         }


# # -----------------------------------------------------------------------------
# # MAIN ROUTE: LIVE MTD vs PREVIOUS-MONTH-SAME-PERIOD BI
# # # -----------------------------------------------------------------------------

# @live_data_bi_bp.route("/live_mtd_bi", methods=["GET"])
# def live_mtd_vs_previous():
#     auth_header = request.headers.get("Authorization")
#     if not auth_header or not auth_header.startswith("Bearer "):
#         return jsonify({"error": "Authorization token is missing or invalid"}), 401

#     token = auth_header.split(" ")[1]

#     try:
#         payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
#         user_id = payload.get("user_id")
#         if not user_id:
#             return jsonify({"error": "Invalid token payload: user_id missing"}), 401

#         country = request.args.get("countryName", "uk")
#         as_of = request.args.get("as_of")

#         # optional custom day range
#         start_day_str = request.args.get("start_day")
#         end_day_str = request.args.get("end_day")
#         try:
#             start_day = int(start_day_str) if start_day_str else None
#             end_day = int(end_day_str) if end_day_str else None
#         except ValueError:
#             start_day = None
#             end_day = None

#         generate_ai_insights = (
#             request.args.get("generate_ai_insights", "false").lower()
#             in ("true", "1", "yes")
#         )

#         ranges = get_mtd_and_prev_ranges(
#             as_of=as_of,
#             start_day=start_day,
#             end_day=end_day,
#         )
#         prev_start = ranges["previous"]["start"]
#         prev_end = ranges["previous"]["end"]
#         curr_start = ranges["current"]["start"]
#         curr_end = ranges["current"]["end"]

#         # âœ… FULL previous month ONLY for graph
#         prev_full_start = date(
#             ranges["meta"]["previous_year"],
#             ranges["meta"]["previous_month"],
#             1
#         )
#         last_day_prev = monthrange(prev_full_start.year, prev_full_start.month)[1]
#         prev_full_end = date(prev_full_start.year, prev_full_start.month, last_day_prev)

#         is_global = country.lower() == "global"
#         key_column = "sku"

#         # 1) fetch previous period (ALIGNED) -> table/growth/totals/AI
#         prev_data_aligned, prev_daily_aligned = fetch_previous_period_data(
#             user_id, country, prev_start, prev_end
#         )

#         # 1b) fetch previous period (FULL MONTH) -> graph only
#         _, prev_daily_full = fetch_previous_period_data(
#             user_id, country, prev_full_start, prev_full_end
#         )

#         # 2) fetch current MTD (âœ… now includes __has_mapping__ in each row)
#         curr_data, curr_daily = fetch_current_mtd_data(
#             user_id, country, curr_start, curr_end, is_global
#         )

#         # 3) growth calculation (use ALIGNED prev)
#         growth_data = calculate_growth(prev_data_aligned, curr_data, key=key_column)

#         # 4) categorization (use ALIGNED prev)
#         prev_keys = {row.get(key_column) for row in prev_data_aligned if row.get(key_column)}

    
#         # ==========================================================
#         # âœ… STEP 5: New / Reviving SKUs ONLY based on previous-period absence
#         # ==========================================================

#         new_reviving = [
#             row for row in growth_data
#             if row.get("new_or_reviving")
#         ]


#         # ==========================================================
#         # âœ… STEP 6: Exclude new_reviving (incl. unmapped) from Top80/Other pipeline
#         # ==========================================================

#         new_reviving_keys = {r.get(key_column) for r in new_reviving if r.get(key_column)}

#         existing = [
#             row
#             for row in growth_data
#             if row.get(key_column) in prev_keys
#             and row.get("Sales Mix (Current)") is not None
#             and row.get(key_column) not in new_reviving_keys   # âœ… exclude
#         ]

#         existing_sorted = sorted(
#             existing, key=lambda x: x["Sales Mix (Current)"], reverse=True
#         )

#         total_sales_mix = sum(
#             row["Sales Mix (Current)"]
#             for row in existing_sorted
#             if row["Sales Mix (Current)"] is not None
#         )

#         cumulative = 0.0
#         top_80_skus, other_skus = [], []

#         for row in existing_sorted:
#             mix = row["Sales Mix (Current)"]
#             if mix is None:
#                 continue
#             proportion = cumulative / total_sales_mix if total_sales_mix else 0
#             if proportion <= 0.8:
#                 top_80_skus.append(row)
#                 cumulative += mix
#             else:
#                 other_skus.append(row)

#         # ------- segment subsets (prev / curr data) -------
#         top_keys = {row.get(key_column) for row in top_80_skus}
#         other_keys = {row.get(key_column) for row in other_skus}
#         new_keys = {row.get(key_column) for row in new_reviving if row.get(key_column)}

#         prev_top = [r for r in prev_data_aligned if r.get(key_column) in top_keys]
#         curr_top = [r for r in curr_data if r.get(key_column) in top_keys]

#         prev_other = [r for r in prev_data_aligned if r.get(key_column) in other_keys]
#         curr_other = [r for r in curr_data if r.get(key_column) in other_keys]

#         prev_new = [r for r in prev_data_aligned if r.get(key_column) in new_keys]
#         curr_new = [r for r in curr_data if r.get(key_column) in new_keys]

#         # ------- segment total rows -------
#         top_80_total_row = build_segment_total_row(
#             prev_top, curr_top, key=key_column, label="Total"
#         )
#         new_reviving_total_row = (
#             build_segment_total_row(
#                 prev_new, curr_new, key=key_column, label="Total"
#             )
#             if new_reviving
#             else None
#         )
#         other_total_row = (
#             build_segment_total_row(
#                 prev_other, curr_other, key=key_column, label="Total"
#             )
#             if other_skus
#             else None
#         )

#         # labels
#         prev_label = (
#             f"{month_abbr[prev_start.month].capitalize()}"
#             f"'{str(prev_start.year)[-2:]} {prev_start.day}â€“{prev_end.day}"
#         )
#         curr_label = (
#             f"{month_abbr[curr_start.month].capitalize()}"
#             f"'{str(curr_start.year)[-2:]} {curr_start.day}â€“{curr_end.day}"
#         )
#         prev_label_full = (
#             f"{month_abbr[prev_full_start.month].capitalize()}"
#             f"'{str(prev_full_start.year)[-2:]} 1â€“{prev_full_end.day}"
#         )

#         # ============================
#         # OVERALL SUMMARY + ACTIONS
#         # ============================
#         prev_totals = aggregate_totals(prev_data_aligned)
#         curr_totals = aggregate_totals(curr_data)

#         all_for_context = growth_data
#         sku_context = build_sku_context(all_for_context, max_items=5)

#         overall = build_ai_summary(
#             prev_totals,
#             curr_totals,
#             top_80_skus,
#             new_reviving,
#             prev_label,
#             curr_label,
#             sku_context=sku_context,
#         )

#         overall_summary = overall.get("summary_bullets", [])
#         overall_actions = overall.get("action_bullets", [])

#         # ============================
#         # SEND EMAIL WITH AI SUMMARY
#         # ============================

#         # 1) Try to read email from JWT payload, else from query param
#         user_email = payload.get("email") or request.args.get("email")
#         if not user_email:
#             user_email = get_user_email_by_id(user_id)


#         if user_email:
#             # 3) Throttle: only once in 24 hours per user+country
#             if has_recent_bi_email(user_id, country, hours=24):
#                 print(f"[INFO] BI email already sent in last 24h for user_id={user_id}, country={country}; skipping.")
#             else:
#                 # 4) Generate a fresh token for this user for deep-link
#                 email_token_payload = {
#                     "user_id": user_id,
#                     "email": user_email,
#                     "scope": "live_mtd_bi",
#                     "exp": datetime.utcnow() + timedelta(hours=24),
#                 }
#                 email_token = jwt.encode(email_token_payload, SECRET_KEY, algorithm="HS256")

#                 try:
#                     send_live_bi_email(
#                         to_email=user_email,
#                         overall_summary=overall_summary,
#                         overall_actions=overall_actions,
#                         country=country,
#                         prev_label=prev_label,
#                         curr_label=curr_label,
#                         deep_link_token=email_token,
#                     )
#                     mark_bi_email_sent(user_id, country)
#                 except Exception as e:
#                     # Don't break the API if email fails
#                     print(f"[WARN] Error sending live BI email: {e}")
#         else:
#             print("[WARN] No user email found in token, query params, or DB; skipping BI email.")



#         # ============================
#         # AI INSIGHTS (SKU-wise)
#         # ============================
#         skus_for_ai = top_80_skus + new_reviving + other_skus
#         insights = {}

#         if generate_ai_insights and skus_for_ai:
#             with ThreadPoolExecutor(max_workers=10) as executor:
#                 future_to_item = {
#                     executor.submit(
#                         generate_live_insight,
#                         item,
#                         country,
#                         is_global,
#                         prev_label,
#                         curr_label,
#                     ): item
#                     for item in skus_for_ai
#                 }

#                 for future in as_completed(future_to_item):
#                     try:
#                         key, result = future.result()
#                         insights[key] = result
#                     except Exception as e:
#                         item = future_to_item[future]
#                         print(
#                             f"Error generating AI insight for SKU={item.get('sku')} "
#                             f"Product={item.get('product_name')}: {e}"
#                         )

#         # ============================
#         # RESPONSE
#         # ============================
#         response_payload = {
#             "message": "Live MTD vs previous-month-same-period comparison",
#             "periods": {
#                 "previous": {
#                     "label": prev_label,
#                     "start_date": prev_start.isoformat(),
#                     "end_date": prev_end.isoformat(),
#                 },
#                 "previous_full": {
#                     "label": prev_label_full,
#                     "start_date": prev_full_start.isoformat(),
#                     "end_date": prev_full_end.isoformat(),
#                 },
#                 "current_mtd": {
#                     "label": curr_label,
#                     "start_date": curr_start.isoformat(),
#                     "end_date": curr_end.isoformat(),
#                 },
#             },
#             "categorized_growth": {
#                 "top_80_skus": top_80_skus,
#                 "top_80_total": top_80_total_row,
#                 "new_or_reviving_skus": new_reviving,  # âœ… includes unmapped now
#                 "new_or_reviving_total": new_reviving_total_row,
#                 "other_skus": other_skus,
#                 "other_total": other_total_row,
#             },
#             "daily_series": {
#                 "previous": prev_daily_full,
#                 "current_mtd": curr_daily,
#             },
#             "daily_series_aligned": {
#                 "previous": prev_daily_aligned,
#                 "current_mtd": curr_daily,
#             },
#             "ai_insights": insights,
#             "overall_summary": overall_summary,
#             "overall_actions": overall_actions,
#         }

#         response_payload = round_numeric_values(response_payload, ndigits=2)
#         return jsonify(response_payload), 200

#     except jwt.ExpiredSignatureError:
#         return jsonify({"error": "Token has expired"}), 401
#     except jwt.InvalidTokenError:
#         return jsonify({"error": "Invalid token"}), 401
#     except Exception as e:
#         print("Unexpected error in /live_mtd_bi:", e)
#         return jsonify({"error": "Server error", "details": str(e)}), 500




from flask import Blueprint, request, jsonify
import jwt
import os
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from config import Config
from calendar import month_name, month_abbr, monthrange
from datetime import date, datetime, timedelta
import pandas as pd
import numpy as np
from openai import OpenAI
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from app.utils.formulas_utils import (
    uk_sales,
    uk_credits,
    uk_profit,
    sku_mask,
    safe_num,
)
from app.utils.email_utils import (
    send_live_bi_email,
    get_user_email_by_id,
    has_recent_bi_email,
    mark_bi_email_sent,
)

# -----------------------------------------------------------------------------
# ENV / DB SETUP
# -----------------------------------------------------------------------------

load_dotenv()
SECRET_KEY = Config.SECRET_KEY

db_url = os.getenv("DATABASE_URL")
db_url2 = os.getenv("DATABASE_AMAZON_URL")

engine_hist = create_engine(db_url)
engine_live = create_engine(db_url2)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
oa_client = OpenAI(api_key=OPENAI_API_KEY)

live_data_bi_bp = Blueprint("live_data_bi_bp", __name__)

# -----------------------------------------------------------------------------
# DATE HELPERS
# -----------------------------------------------------------------------------

def is_blank_str(x):
    return x is None or (isinstance(x, str) and x.strip() == "")


def fetch_sku_product_mapping(user_id: int) -> pd.DataFrame:
    """
    sku_{user_id}_data_table se sku_uk -> product_name mapping uthata hai.
    Sirf VALID mappings return karega:
      - sku_uk present ho
      - product_name present ho (non-null, non-empty)
    """
    table_name = f"sku_{user_id}_data_table"

    query = text(f"""
        SELECT
            sku_uk,
            product_name
        FROM {table_name}
    """)

    with engine_hist.connect() as conn:
        df = pd.read_sql(query, conn)

    # null / duplicate clean up
    if df.empty:
        return df

    # âœ… sku must exist
    df = df.dropna(subset=["sku_uk"])

    # âœ… product_name must exist and must not be blank
    df = df.dropna(subset=["product_name"])
    df["product_name"] = df["product_name"].astype(str)
    df = df[df["product_name"].str.strip() != ""]

    # âœ… normalize sku column for joining
    df = df.rename(columns={"sku_uk": "sku"})

    # optional: sku as string (safer for joins if orders.sku is string)
    df["sku"] = df["sku"].astype(str).str.strip()

    # âœ… remove duplicates
    df = df.drop_duplicates(subset=["sku"])

    return df

def clamp_near_zero(value, eps=1e-9):
    if value is None:
        return value
    return 0.0 if abs(value) < eps else value



def get_mtd_and_prev_ranges(as_of=None, start_day=None, end_day=None):
    """
    Default:
      - current: current month 1st -> today (MTD)
      - previous: previous month 1st -> previous month LAST day (full month)

    Agar start_day & end_day diye gaye hain (frontend date range se):
      - current: current month [start_day, end_day] (clamped to month length & today)
      - previous: previous month [start_day, end_day] (clamped to that month length)
    """
    # --- resolve today / as_of ---
    if as_of is None:
        today = date.today()
    else:
        if isinstance(as_of, str):
            today = datetime.strptime(as_of, "%Y-%m-%d").date()
        elif isinstance(as_of, date):
            today = as_of
        else:
            today = date.today()

    # prev month/year
    if today.month == 1:
        prev_month = 12
        prev_year = today.year - 1
    else:
        prev_month = today.month - 1
        prev_year = today.year

    # ---------- custom day range ----------
    if start_day and end_day:
        sd = int(min(start_day, end_day))
        ed = int(max(start_day, end_day))

        # current month clamp
        last_day_curr = monthrange(today.year, today.month)[1]
        sd_curr = max(1, min(sd, last_day_curr))
        # end ko month & today dono se clamp kar do (future days avoid)
        ed_curr = max(1, min(ed, last_day_curr, today.day))

        current_period_start = date(today.year, today.month, sd_curr)
        current_period_end = date(today.year, today.month, ed_curr)

        # previous month clamp
        last_day_prev = monthrange(prev_year, prev_month)[1]
        sd_prev = max(1, min(sd, last_day_prev))
        ed_prev = max(1, min(ed, last_day_prev))

        prev_month_start = date(prev_year, prev_month, sd_prev)
        prev_month_end = date(prev_year, prev_month, ed_prev)

    # ---------- default behaviour (no range) ----------
    else:
        current_period_start = date(today.year, today.month, 1)
        current_period_end = today

        prev_month_start = date(prev_year, prev_month, 1)
        last_day_prev = monthrange(prev_year, prev_month)[1]

        # âœ… Align previous end day to current MTD day (clamp to prev month length)
        prev_end_day = min(today.day, last_day_prev)
        prev_month_end = date(prev_year, prev_month, prev_end_day)

    return {
        "current": {
            "start": current_period_start,
            "end": current_period_end,
        },
        "previous": {
            "start": prev_month_start,
            "end": prev_month_end,
        },
        "meta": {
            "today": today,
            "current_month": today.month,
            "current_year": today.year,
            "previous_month": prev_month,
            "previous_year": prev_year,
        },
    }



def month_num_to_name(m):
    try:
        m_int = int(m)
        return month_name[m_int].lower() if 1 <= m_int <= 12 else None
    except Exception:
        return None

def construct_prev_table_name(user_id, country, month, year):
    month_str = month_num_to_name(month)
    if not month_str:
        raise ValueError("Invalid month")
    return f"user_{user_id}_{country.lower()}_{month_str}{year}_data"

# -----------------------------------------------------------------------------
# ðŸ”¹ NEW HELPER â€” HISTORIC BI PARITY (6-MONTH LOOKBACK)
# -----------------------------------------------------------------------------

def fetch_historical_skus_last_6_months(user_id: int, country: str, ref_date: date):
    """
    Mirrors Historic BI logic.
    Returns set of SKUs seen in last 6 months (excluding current month).
    """
    skus = set()
    y, m = ref_date.year, ref_date.month

    for _ in range(6):
        m -= 1
        if m == 0:
            m = 12
            y -= 1

        try:
            table = construct_prev_table_name(
                user_id=user_id,
                country=country,
                month=m,
                year=y
            )
        except Exception:
            continue

        try:
            with engine_hist.connect() as conn:
                res = conn.execute(text(f"SELECT DISTINCT sku FROM {table}"))
                for r in res:
                    if r[0]:
                        skus.add(str(r[0]).strip())
        except Exception:
            continue

    return skus

def normalize_sales_mix(df: pd.DataFrame, mix_col="sales_mix", digits=2):
    """
    Forces sales_mix to sum exactly to 100.00 after rounding.
    """
    if df.empty or mix_col not in df.columns:
        return df

    df = df.copy()

    # Round all values
    df[mix_col] = df[mix_col].round(digits)

    total = df[mix_col].sum()
    diff = round(100.0 - total, digits)

    if abs(diff) > 0:
        # Add residual to the SKU with highest mix (or last row)
        idx = df[mix_col].idxmax()
        df.loc[idx, mix_col] = round(df.loc[idx, mix_col] + diff, digits)

    return df


def compute_sku_metrics_from_df(df: pd.DataFrame) -> list:
    """
    Given a raw settlement-style DataFrame with columns like:

      sku, quantity, product_sales, cost_of_unit_sold, etc.

    compute per-SKU metrics:

      quantity
      product_sales
      net_sales
      profit
      asp
      unit_wise_profitability
      sales_mix
      product_name
    """
    if df is None or df.empty:
        return []

    df = df.copy()

    # Keep only valid SKUs (same rule as formula_utils)
    if "sku" in df.columns:
        df = df.loc[sku_mask(df)]

    if df.empty:
        return []

    # ---- quantity per SKU ----
    if "quantity" in df.columns:
        qty_df = (
            df.assign(quantity=safe_num(df["quantity"]))
              .groupby("sku", as_index=False)["quantity"]
              .sum()
        )
    else:
        qty_df = pd.DataFrame(columns=["sku", "quantity"])

    # ---- product_sales per SKU (GROSS SALES) ----
    if "product_sales" in df.columns:
        product_sales_df = (
            df.assign(product_sales=safe_num(df["product_sales"]))
              .groupby("sku", as_index=False)["product_sales"]
              .sum()
        )
    else:
        product_sales_df = pd.DataFrame(columns=["sku", "product_sales"])

    # ---- product_name per SKU (first non-null) ----
    if "product_name" in df.columns:
        name_df = (
            df[["sku", "product_name"]]
            .dropna(subset=["sku"])
            .groupby("sku", as_index=False)
            .first()
        )
    else:
        name_df = pd.DataFrame(columns=["sku", "product_name"])

    # ---- sales, credits, profit per SKU via formula_utils ----
    _, sales_by, _ = uk_sales(df)
    _, credits_by, _ = uk_credits(df)
    _, profit_by, _ = uk_profit(df)

    if not sales_by.empty:
        sales_by = sales_by.rename(columns={"__metric__": "sales_metric"})
    if not credits_by.empty:
        credits_by = credits_by.rename(columns={"__metric__": "credits_metric"})
    if not profit_by.empty:
        profit_by = profit_by.rename(columns={"__metric__": "profit_metric"})

    # ---- merge everything ----
    metrics = (
        qty_df
        .merge(name_df, on="sku", how="left")
        .merge(product_sales_df, on="sku", how="left")  # âœ… ADD
        .merge(
            sales_by[["sku", "sales_metric"]]
            if not sales_by.empty
            else pd.DataFrame(columns=["sku", "sales_metric"]),
            on="sku", how="left"
        )
        .merge(
            credits_by[["sku", "credits_metric"]]
            if not credits_by.empty
            else pd.DataFrame(columns=["sku", "credits_metric"]),
            on="sku", how="left"
        )
        .merge(
            profit_by[["sku", "profit_metric"]]
            if not profit_by.empty
            else pd.DataFrame(columns=["sku", "profit_metric"]),
            on="sku", how="left"
        )
    )

    # ---- compute final fields ----
    metrics["quantity"] = safe_num(metrics["quantity"])
    metrics["product_sales"] = safe_num(metrics.get("product_sales", 0.0))
    metrics["sales_metric"] = safe_num(metrics.get("sales_metric", 0.0))
    metrics["credits_metric"] = safe_num(metrics.get("credits_metric", 0.0))
    metrics["profit_metric"] = safe_num(metrics.get("profit_metric", 0.0))

    metrics["net_sales"] = metrics["sales_metric"]
    metrics["profit"] = metrics["profit_metric"]

    # asp & per-unit profitability
    qty_nonzero = metrics["quantity"].replace(0, np.nan)
    metrics["asp"] = metrics["net_sales"] / qty_nonzero
    metrics["unit_wise_profitability"] = metrics["profit"] / qty_nonzero

    # sales_mix (% of net_sales)
    total_net_sales = float(metrics["net_sales"].sum())
    if total_net_sales != 0:
        metrics["sales_mix"] = (metrics["net_sales"] / total_net_sales) * 100.0
    else:
        metrics["sales_mix"] = 0.0

    # âœ… force exact 100%
    metrics = normalize_sales_mix(metrics, "sales_mix", digits=2)

    # final list of dicts expected by growth logic
    out_cols = [
        "sku",
        "product_name",
        "quantity",
        "product_sales",            # âœ… ADD
        "asp",
        "profit",
        "sales_mix",
        "net_sales",
        "unit_wise_profitability",
    ]

    return (
        metrics[out_cols]
        .replace({np.nan: None})
        .to_dict(orient="records")
    )



# -----------------------------------------------------------------------------
# FETCH DATA: PREVIOUS PERIOD (historical table) + CURRENT MTD (orders)
# -----------------------------------------------------------------------------

# def fetch_previous_period_data(user_id, country, prev_start: date, prev_end: date):
#     """
#     Return:
#       sku_metrics: list of per-SKU metrics (for growth calc)
#       daily_series: list of {date, quantity, net_sales} for line chart
#     """
#     table_name = construct_prev_table_name(
#         user_id=user_id,
#         country=country,
#         month=prev_start.month,
#         year=prev_start.year,
#     )

#     # Debug: show constructed table name
#     print(f"[DEBUG] Previous Period Table: {table_name}")

#     # Safer approach:
#     #  - build a subquery that casts date_time to date_ts
#     #  - use NULLIF to avoid casting '0' / '' values
#     query = text(f"""
#         SELECT *
#         FROM (
#             SELECT
#                 *,
#                 NULLIF(NULLIF(date_time, '0'), '')::timestamp AS date_ts
#             FROM {table_name}
#         ) t
#         WHERE date_ts >= :start_date
#           AND date_ts < :end_date_plus_one
#     """)

#     params = {
#         "start_date": datetime.combine(prev_start, datetime.min.time()),
#         "end_date_plus_one": datetime.combine(prev_end + timedelta(days=1), datetime.min.time()),
#     }

#     # Debug: show SQL + params
#     print("[DEBUG] SQL for previous period:")
#     print(query)
#     print("[DEBUG] Params:", params)

#     with engine_hist.connect() as conn:
#         result = conn.execute(query, params)
#         rows = result.fetchall()

#         # Debug: row count
#         print(f"[DEBUG] Fetched {len(rows)} rows from {table_name}")

#         if len(rows) > 0:
#             # Show first 2 rows for sanity
#             print("[DEBUG] First rows:", rows[:2])

#         if not rows:
#             print("[DEBUG] No previous period data found.")
#             return [], []

#         df = pd.DataFrame(rows, columns=result.keys())

#     # Debug: DataFrame shape
#     print(f"[DEBUG] DataFrame shape: {df.shape}")
#     print(f"[DEBUG] DataFrame columns: {list(df.columns)}")

#     # ---- per-SKU metrics (existing behaviour) ----
#     sku_metrics = compute_sku_metrics_from_df(df)

#     # Debug: Number of SKU items returned
#     print(f"[DEBUG] SKU Metrics Count: {len(sku_metrics)}")

#         # ---- daily series for line chart (quantity + net_sales + product_sales + profit) ----
#     daily_series = []

#     # Prefer using the parsed timestamp column if present
#     date_col_for_series = None
#     if "date_ts" in df.columns:
#         date_col_for_series = "date_ts"
#     elif "date_time" in df.columns:
#         date_col_for_series = "date_time"

#     if date_col_for_series:
#         tmp = df.copy()
#         tmp["date_only"] = pd.to_datetime(tmp[date_col_for_series], errors="coerce").dt.date
#         tmp = tmp.dropna(subset=["date_only"])

#         # 1) quantity/day (simple sum)
#         if "quantity" in tmp.columns:
#             tmp["quantity"] = safe_num(tmp["quantity"])
#             daily_qty = tmp.groupby("date_only", as_index=False)["quantity"].sum()
#             qty_map = {d: float(v) for d, v in zip(daily_qty["date_only"], daily_qty["quantity"])}
#         else:
#             qty_map = {}

#         # 2) product_sales/day (gross sales)
#         if "product_sales" in tmp.columns:
#             tmp["product_sales"] = safe_num(tmp["product_sales"])
#             daily_ps = tmp.groupby("date_only", as_index=False)["product_sales"].sum()
#             ps_map = {d: float(v) for d, v in zip(daily_ps["date_only"], daily_ps["product_sales"])}
#         else:
#             ps_map = {}

#         # 3) net_sales/day using uk_sales (needs sku + date_only)
#         net_sales_map = {}
#         try:
#             df_sales = tmp.copy()
#             df_sales["sku"] = df_sales.get("sku", "").astype(str).str.strip()
#             sales_src, sales_by, _ = uk_sales(df_sales)   # sales_by: sku, __metric__
#             if sales_src is not None and not sales_src.empty:
#                 # sales_src is usually row-level; safest is to aggregate on (date_only, sku) if available
#                 # If sales_src already has date column, use it; otherwise compute from df_sales alignment.
#                 pass

#             if sales_by is not None and not sales_by.empty:
#                 # We need DAILY, so compute metric per row group.
#                 # Best reliable way: recompute uk_sales on each day-slice is expensive.
#                 # Instead: compute sales metric per row by building "sales_metric_row" if uk_sales returns it.
#                 # If uk_sales does not return row-level, fallback to using product_sales as net_sales.
#                 raise RuntimeError("uk_sales returned only sku-level; falling back to product_sales for daily net_sales.")
#         except Exception:
#             # fallback: daily net_sales = daily product_sales (matches your older behavior)
#             net_sales_map = dict(ps_map)

#         # 4) profit/day using uk_profit (same limitation as above; fallback if only sku-level)
#         profit_map = {}
#         try:
#             df_profit = tmp.copy()
#             df_profit["sku"] = df_profit.get("sku", "").astype(str).str.strip()
#             profit_src, profit_by, _ = uk_profit(df_profit)
#             if profit_src is not None and not profit_src.empty:
#                 # If profit_src contains per-row metric, aggregate daily.
#                 if "__metric__" in profit_src.columns:
#                     profit_src = profit_src.copy()
#                     # ensure date is present
#                     if "date_only" not in profit_src.columns:
#                         # align date_only from tmp if profit_src is same-row as input
#                         profit_src["date_only"] = df_profit["date_only"].values
#                     profit_src["__metric__"] = safe_num(profit_src["__metric__"])
#                     daily_profit = profit_src.groupby("date_only", as_index=False)["__metric__"].sum()
#                     profit_map = {d: float(v) for d, v in zip(daily_profit["date_only"], daily_profit["__metric__"])}
#                 else:
#                     raise RuntimeError("profit_src missing __metric__")
#             else:
#                 # if only sku-level, can't do daily without row-level; fallback to 0
#                 profit_map = {}
#         except Exception:
#             profit_map = {}

#         all_dates = sorted(set(qty_map.keys()) | set(ps_map.keys()) | set(net_sales_map.keys()) | set(profit_map.keys()))

#         for d in all_dates:
#             daily_series.append(
#                 {
#                     "date": d.isoformat(),
#                     "quantity": qty_map.get(d, 0.0) if qty_map else None,
#                     "net_sales": net_sales_map.get(d, 0.0) if net_sales_map else None,
#                     "product_sales": ps_map.get(d, 0.0) if ps_map else None,
#                     "profit": profit_map.get(d, 0.0) if profit_map else None,
#                 }
#             )

#         print(f"[DEBUG] Daily series points: {len(daily_series)}")


#     return sku_metrics, daily_series

def fetch_previous_period_data(user_id, country, prev_start: date, prev_end: date):
    """
    Return:
      sku_metrics: list of per-SKU metrics (for growth calc)
      daily_series: list of {date, quantity, net_sales, product_sales, profit}
    """
    table_name = construct_prev_table_name(
        user_id=user_id,
        country=country,
        month=prev_start.month,
        year=prev_start.year,
    )

    print(f"[DEBUG] Previous Period Table: {table_name}")

    query = text(f"""
        SELECT *
        FROM (
            SELECT
                *,
                NULLIF(NULLIF(date_time, '0'), '')::timestamp AS date_ts
            FROM {table_name}
        ) t
        WHERE date_ts >= :start_date
          AND date_ts < :end_date_plus_one
    """)

    params = {
        "start_date": datetime.combine(prev_start, datetime.min.time()),
        "end_date_plus_one": datetime.combine(prev_end + timedelta(days=1), datetime.min.time()),
    }

    print("[DEBUG] SQL for previous period:")
    print(query)
    print("[DEBUG] Params:", params)

    with engine_hist.connect() as conn:
        result = conn.execute(query, params)
        rows = result.fetchall()

        print(f"[DEBUG] Fetched {len(rows)} rows from {table_name}")

        if rows:
            print("[DEBUG] First rows:", rows[:2])
        else:
            print("[DEBUG] No previous period data found.")
            return [], []

        df = pd.DataFrame(rows, columns=result.keys())

    print(f"[DEBUG] DataFrame shape: {df.shape}")
    print(f"[DEBUG] DataFrame columns: {list(df.columns)}")

    # -------------------------------------------------
    # 1) PER-SKU METRICS (UNCHANGED)
    # -------------------------------------------------
    sku_metrics = compute_sku_metrics_from_df(df)
    print(f"[DEBUG] SKU Metrics Count: {len(sku_metrics)}")

    # -------------------------------------------------
    # 2) DAILY SERIES (FORMULA UTILS DAY-WISE)
    # -------------------------------------------------
    daily_series = []

    date_col = "date_ts" if "date_ts" in df.columns else "date_time"

    if date_col in df.columns:
        tmp = df.copy()

        # âœ… NEW: filter out rows with invalid / blank SKUs
        if "sku" in tmp.columns:
            tmp = tmp.loc[sku_mask(tmp)]

        tmp["date_only"] = pd.to_datetime(tmp[date_col], errors="coerce").dt.date
        tmp = tmp.dropna(subset=["date_only"])

        for d, day_df in tmp.groupby("date_only"):

            quantity = (
                safe_num(day_df["quantity"]).sum()
                if "quantity" in day_df.columns
                else 0.0
            )

            product_sales = (
                safe_num(day_df["product_sales"]).sum()
                if "product_sales" in day_df.columns
                else 0.0
            )

            net_sales, _, _ = uk_sales(day_df)
            profit, _, _ = uk_profit(day_df)

            daily_series.append({
                "date": d.isoformat(),
                "quantity": float(quantity),
                "product_sales": float(product_sales),
                "net_sales": float(net_sales),
                "profit": float(profit),
            })

        daily_series = sorted(daily_series, key=lambda x: x["date"])
        print(f"[DEBUG] Daily series points: {len(daily_series)}")

    # -------------------------------------------------
    # 3) DEBUG TOTALS FOR CROSS-CHECKING
    # -------------------------------------------------
    total_daily_qty = sum(d["quantity"] for d in daily_series)
    total_daily_ps = sum(d["product_sales"] for d in daily_series)
    total_daily_ns = sum(d["net_sales"] for d in daily_series)
    total_daily_profit = sum(d["profit"] for d in daily_series)

    full_ns, _, _ = uk_sales(df)
    full_profit, _, _ = uk_profit(df)

    print("\n================ DAILY SERIES CHECK =================")
    print(f"Daily SUM â†’ Quantity       : {total_daily_qty:.2f}")
    print(f"Daily SUM â†’ Product Sales  : {total_daily_ps:.2f}")
    print(f"Daily SUM â†’ Net Sales      : {total_daily_ns:.2f}")
    print(f"Daily SUM â†’ Profit         : {total_daily_profit:.2f}")

    print("\n--------------- FULL PERIOD (formula_utils) ---------")
    print(f"Full Period â†’ Net Sales    : {full_ns:.2f}")
    print(f"Full Period â†’ Profit       : {full_profit:.2f}")

    print("\n--------------- DIFFERENCE (Daily - Full) -----------")
    print(f"Net Sales Diff  : {(total_daily_ns - full_ns):.2f}")
    print(f"Profit Diff     : {(total_daily_profit - full_profit):.2f}")
    print("====================================================\n")

    return sku_metrics, daily_series



def fetch_current_mtd_data(user_id, country, curr_start: date, curr_end: date):
    """
    Return:
      sku_metrics: list of per-SKU metrics (manual aggregation, no formula utils)
      daily_series: list of {date, quantity, net_sales} for line chart

    Notes:
      - country and is_global are not used (kept in signature to avoid breaking callers).
      - Liveorders table renamed to liveorders.
      - liveorders does NOT have product_name or country columns.
      - product_name in output is derived from sku (fallback) and optionally overridden via SKU mapping.
      - Uses profit and cogs directly (assumes cogs is TOTAL per row).
      - net_sales = product_sales + promotional_rebates
    """
    table_name = "liveorders"

    query = text(f"""
        SELECT
            sku,
            quantity,
            cogs,
            product_sales,
            promotional_rebates,
            profit,
            purchase_date,
            order_status
        FROM {table_name}
        WHERE user_id = :user_id
          AND purchase_date >= :start_date
          AND purchase_date < :end_date_plus_one
    """)

    params = {
        "user_id": user_id,
        "start_date": datetime.combine(curr_start, datetime.min.time()),
        "end_date_plus_one": datetime.combine(curr_end + timedelta(days=1), datetime.min.time()),
    }

    print("\n[DEBUG] CURRENT MTD QUERY")
    print(query)
    print("[DEBUG] Params:", params)

    with engine_live.connect() as conn:
        result = conn.execute(query, params)
        rows = result.fetchall()
        print(f"[DEBUG] Current MTD rows fetched (full filter): {len(rows)}")

        if not rows:
            print("[DEBUG] No current MTD data found in liveorders table.")

            diag1 = conn.execute(text(f"""
                SELECT MIN(purchase_date), MAX(purchase_date), COUNT(*)
                FROM {table_name}
                WHERE user_id = :user_id
            """), {"user_id": user_id}).fetchone()
            print("[DEBUG] User-level date range in liveorders:", diag1)

            diag2 = conn.execute(text(f"""
                SELECT DISTINCT order_status
                FROM {table_name}
                WHERE user_id = :user_id
            """), {"user_id": user_id}).fetchall()
            print("[DEBUG] Order statuses for this user in liveorders:", diag2)

            return [], []

        df = pd.DataFrame(rows, columns=result.keys())

    # âœ… fallback "product_name" = sku (so downstream/UI doesn't break)
    df["sku"] = df["sku"].astype(str).str.strip()
    df["product_name"] = df["sku"]

    # âœ… default assume "unmapped"
    df["__has_mapping__"] = False

    # ðŸ”¹ SKU mapping se product_name override + __has_mapping__ flag set
    try:
        sku_map_df = fetch_sku_product_mapping(user_id)  # only valid mappings
        if not sku_map_df.empty:
            print(f"[DEBUG] Merging SKU mapping for user_id={user_id}")

            sku_map_df = sku_map_df.copy()
            sku_map_df["sku"] = sku_map_df["sku"].astype(str).str.strip()

            mapped_skus = set(sku_map_df["sku"].dropna())
            df["__has_mapping__"] = df["sku"].isin(mapped_skus)

            df = df.merge(
                sku_map_df,
                on="sku",
                how="left",
                suffixes=("", "_from_sku_table"),
            )

            if "product_name_from_sku_table" in df.columns:
                df["product_name"] = df["product_name_from_sku_table"].combine_first(df["product_name"])
                df.drop(columns=["product_name_from_sku_table"], inplace=True)
        else:
            print("[DEBUG] SKU mapping DF empty, using SKU as product_name.")
    except Exception as e:
        print("[WARN] Failed to fetch/merge SKU product mapping:", e)

    # ----------------------------
    # âœ… Manual prep (no formula utils)
    # ----------------------------
    df["quantity"] = safe_num(df.get("quantity", 0))
    df["profit"] = safe_num(df.get("profit", 0))
    df["cogs"] = safe_num(df.get("cogs", 0))  # assumed TOTAL COGS per row

    df["product_sales"] = safe_num(df.get("product_sales", 0))
    df["promotional_rebates"] = safe_num(df.get("promotional_rebates", 0))

    # âœ… net_sales = product_sales + promotional_rebates
    df["net_sales"] = df["product_sales"] + df["promotional_rebates"]

    # ----------------------------
    # ---- per-SKU metrics (manual but compatible with growth pipeline) ----
    # ----------------------------
    sku_agg = (
        df.groupby("sku", as_index=False)
          .agg(
              product_name=("product_name", "first"),  # now sku fallback or mapped name
              quantity=("quantity", "sum"),
              net_sales=("net_sales", "sum"),
              product_sales=("product_sales", "sum"), 
              profit=("profit", "sum"),
              cogs=("cogs", "sum"),
              __has_mapping__=("__has_mapping__", "max"),
          )
    )

    qty_nonzero = sku_agg["quantity"].replace(0, np.nan)

    # Required by existing growth + AI logic
    sku_agg["asp"] = (sku_agg["net_sales"] / qty_nonzero).fillna(0.0)
    sku_agg["unit_wise_profitability"] = (sku_agg["profit"] / qty_nonzero).fillna(0.0)

    total_net_sales = float(sku_agg["net_sales"].sum())
    if total_net_sales:
        sku_agg["sales_mix"] = (sku_agg["net_sales"] / total_net_sales) * 100.0
    else:
        sku_agg["sales_mix"] = 0.0

    # âœ… force exact 100%
    sku_agg = normalize_sales_mix(sku_agg, "sales_mix", digits=2)


    # Your requested metric
    sku_agg["profit_per_unit"] = sku_agg["unit_wise_profitability"]

    sku_metrics = sku_agg.to_dict(orient="records")

    # ----------------------------
    # ---- daily series for line chart (quantity + net_sales + product_sales + profit) ----
    # ----------------------------
    daily_series = []
    if "purchase_date" in df.columns:
        tmp = df.copy()
        tmp["date_only"] = pd.to_datetime(tmp["purchase_date"]).dt.date

        daily_qty = tmp.groupby("date_only", as_index=False)["quantity"].sum()
        qty_map = {d: float(q) for d, q in zip(daily_qty["date_only"], daily_qty["quantity"])}

        daily_ns = tmp.groupby("date_only", as_index=False)["net_sales"].sum()
        ns_map = {d: float(v) for d, v in zip(daily_ns["date_only"], daily_ns["net_sales"])}

        daily_ps = tmp.groupby("date_only", as_index=False)["product_sales"].sum()
        ps_map = {d: float(v) for d, v in zip(daily_ps["date_only"], daily_ps["product_sales"])}

        daily_profit = tmp.groupby("date_only", as_index=False)["profit"].sum()
        profit_map = {d: float(v) for d, v in zip(daily_profit["date_only"], daily_profit["profit"])}

        for d in sorted(set(qty_map.keys()) | set(ns_map.keys()) | set(ps_map.keys()) | set(profit_map.keys())):
            daily_series.append(
                {
                    "date": d.isoformat(),
                    "quantity": qty_map.get(d, 0.0),
                    "net_sales": ns_map.get(d, 0.0),
                    "product_sales": ps_map.get(d, 0.0),
                    "profit": profit_map.get(d, 0.0),
                }
            )


    return sku_metrics, daily_series




# ----------------------------------------------------------------------------
# GROWTH METRIC CALCULATION (same formulas as Business Insights)
# -----------------------------------------------------------------------------

growth_field_mapping = {
    "quantity": "Unit Growth (%)",
    "asp": "ASP Growth (%)",
    "net_sales": "Net Sales Growth (%)",
    "product_sales": "Gross Sales Growth (%)",
    "sales_mix": "Sales Mix Change (%)",
    "unit_wise_profitability": "Profit Per Unit (%)",
    "profit": "CM1 Profit Impact (%)",
}


def categorize_growth(value):
    if value is None:
        return "No Data"
    if value >= 5:
        return "High Growth"
    elif value > 0.5:
        return "Low Growth"
    elif value < -0.5:
        return "Negative Growth"
    else:
        return "No Growth"


def safe_float_local(val):
    try:
        if val is None:
            return None
        return float(val)
    except (ValueError, TypeError):
        return None

def round_numeric_values(obj, ndigits=2):
    """
    Recursively walk any dict/list and:
    - round floats to `ndigits`
    - convert None / NaN to 0.0 (so UI blanks don't appear)
    """

    # âœ… None -> 0
    if obj is None:
        return 0.0

    # âœ… NaN (python float) -> 0
    if isinstance(obj, float) and np.isnan(obj):
        return 0.0

    # float -> round
    if isinstance(obj, float):
        return round(obj, ndigits)

    # numpy floats -> round (NaN safe)
    if isinstance(obj, (np.floating,)):
        v = float(obj)
        if np.isnan(v):
            return 0.0
        return round(v, ndigits)

    # numpy ints -> normal int
    if isinstance(obj, (np.integer,)):
        return int(obj)

    # dict -> recurse values
    if isinstance(obj, dict):
        return {k: round_numeric_values(v, ndigits) for k, v in obj.items()}

    # list/tuple -> recurse items
    if isinstance(obj, (list, tuple)):
        return [round_numeric_values(v, ndigits) for v in obj]

    # strings/bool/etc as-is
    return obj


def build_segment_total_row(prev_segment, curr_segment, key="sku", label="Total"):
    """
    prev_segment / curr_segment: subset of prev_data / curr_data
    (sirf woh SKUs jo top_80 me hain, etc.)

    Return: ek row jaisa calculate_growth deta hai, bas aggregated.
    """
    # ---- totals for quantity / net_sales / profit ----
    prev_qty = prev_net = prev_prof = 0.0
    curr_qty = curr_net = curr_prof = 0.0

    # previous
    for r in prev_segment:
        q = safe_float_local(r.get("quantity"))
        s = safe_float_local(r.get("net_sales"))
        p = safe_float_local(r.get("profit"))
        if q is not None: prev_qty += q
        if s is not None: prev_net += s
        if p is not None: prev_prof += p

    # current
    for r in curr_segment:
        q = safe_float_local(r.get("quantity"))
        s = safe_float_local(r.get("net_sales"))
        p = safe_float_local(r.get("profit"))
        if q is not None: curr_qty += q
        if s is not None: curr_net += s
        if p is not None: curr_prof += p

    # ASP / unit profit indexes (portfolio level)
    prev_asp = prev_net / prev_qty if prev_qty else None
    curr_asp = curr_net / curr_qty if curr_qty else None

    prev_up = prev_prof / prev_qty if prev_qty else None
    curr_up = curr_prof / curr_qty if curr_qty else None

    # sales mix (sum of sku-wise mix)
    prev_mix = sum(
        (safe_float_local(r.get("sales_mix")) or 0.0) for r in prev_segment
    )
    curr_mix = sum(
        (safe_float_local(r.get("sales_mix")) or 0.0) for r in curr_segment
    )

    # pseudo SKU rows
    seg_id = f"{label.upper().replace(' ', '_')}_SEGMENT"

    prev_row = {
        key: seg_id,
        "product_name": label,
        "quantity": prev_qty,
        "net_sales": prev_net,
        "profit": prev_prof,
        "asp": prev_asp,
        "unit_wise_profitability": prev_up,
        "sales_mix": prev_mix,
    }

    curr_row = {
        key: seg_id,
        "product_name": label,
        "quantity": curr_qty,
        "net_sales": curr_net,
        "profit": curr_prof,
        "asp": curr_asp,
        "unit_wise_profitability": curr_up,
        "sales_mix": curr_mix,
    }

    # existing logic reuse
    seg_growth_list = calculate_growth([prev_row], [curr_row], key=key)
    return seg_growth_list[0] if seg_growth_list else None

def calc_profit_pct(profit, net_sales):
    profit = safe_float_local(profit)
    net_sales = safe_float_local(net_sales)

    if profit is None or net_sales is None or net_sales == 0:
        return 0.0

    return round((profit / net_sales) * 100.0, 2)

def calculate_growth(prev_data, curr_data, key="sku", numeric_fields=None) -> list:
    """
    prev_data / curr_data: list[dict] with keys
      quantity, asp, net_sales, sales_mix, unit_wise_profitability, profit, product_name, sku

    Returns, per SKU:
      - <field>_prev  (previous period raw value)
      - <field>_curr  (current MTD raw value)
      - % growth mapped via growth_field_mapping
      - Sales Mix (Current)  (for categorization / frontend)
      - new_or_reviving flag
      - profit_pct_prev / profit_pct_curr  âœ… NEW (NO growth)
    """
    if numeric_fields is None:
        numeric_fields = list(growth_field_mapping.keys())

    prev_dict = {row.get(key): row for row in prev_data if row.get(key)}
    results = []

    for row2 in curr_data:
        item_key = row2.get(key)
        if not item_key:
            continue

        growth_row = {
            "product_name": row2.get("product_name"),
            key: item_key,
        }

        # ---- Month2 / current sales mix ----
        sales_mix_curr = safe_float_local(row2.get("sales_mix"))
        growth_row["Sales Mix (Current)"] = round(sales_mix_curr, 2) if sales_mix_curr is not None else 0.0

        # -----------------------------
        # Case 1: Existing SKU (in prev_data)
        # -----------------------------
        if item_key in prev_dict:
            row1 = prev_dict[item_key]

            for field in numeric_fields:
                val1 = safe_float_local(row1.get(field))  # previous period
                val2 = safe_float_local(row2.get(field))  # current period

                # âœ… raw values for Excel/UI (no blanks)
                growth_row[f"{field}_prev"] = 0.0 if val1 is None else val1
                growth_row[f"{field}_curr"] = 0.0 if val2 is None else val2

                # âœ… Growth calculation
                if field == "sales_mix":
                    if val1 is None or val2 is None:
                        growth = 0.0
                    else:
                        raw_change = val2 - val1
                        raw_change = clamp_near_zero(raw_change)
                        growth = round(raw_change, 2)
                else:
                    if val1 is None or val2 is None:
                        growth = 0.0
                    elif val1 != 0:
                        growth = round(((val2 - val1) / val1) * 100.0, 2)
                    else:
                        growth = 0.0

                label = growth_field_mapping[field]
                growth_row[label] = {
                    "category": categorize_growth(growth),
                    "value": growth,
                }

            # âœ… PROFIT % (NO growth, absolute value)
            profit_prev = safe_float_local(row1.get("profit"))
            sales_prev = safe_float_local(row1.get("net_sales"))
            profit_curr = safe_float_local(row2.get("profit"))
            sales_curr = safe_float_local(row2.get("net_sales"))

            growth_row["profit_pct_prev"] = (
                round((profit_prev / sales_prev) * 100.0, 2)
                if profit_prev is not None and sales_prev not in (None, 0)
                else 0.0
            )
            growth_row["profit_pct_curr"] = (
                round((profit_curr / sales_curr) * 100.0, 2)
                if profit_curr is not None and sales_curr not in (None, 0)
                else 0.0
            )

        # -----------------------------
        # Case 2: New / Reviving SKU
        # -----------------------------
        else:
            growth_row["new_or_reviving"] = True
            row1 = prev_dict.get(item_key, {})

            for field in numeric_fields:
                val1 = safe_float_local(row1.get(field))
                val2 = safe_float_local(row2.get(field))

                growth_row[f"{field}_prev"] = 0.0 if val1 is None else val1
                growth_row[f"{field}_curr"] = 0.0 if val2 is None else val2

                label = growth_field_mapping[field]

                if val1 is not None and val1 > 0 and val2 is not None:
                    growth = round(((val2 - val1) / val1) * 100.0, 2)
                    growth_row[label] = {
                        "category": categorize_growth(growth),
                        "value": growth,
                    }
                else:
                    growth_row[label] = {
                        "category": "No Data",
                        "value": 0.0,
                    }

            # âœ… PROFIT % (NO growth, absolute value)
            profit_prev = safe_float_local(row1.get("profit"))
            sales_prev = safe_float_local(row1.get("net_sales"))
            profit_curr = safe_float_local(row2.get("profit"))
            sales_curr = safe_float_local(row2.get("net_sales"))

            growth_row["profit_pct_prev"] = (
                round((profit_prev / sales_prev) * 100.0, 2)
                if profit_prev is not None and sales_prev not in (None, 0)
                else 0.0
            )
            growth_row["profit_pct_curr"] = (
                round((profit_curr / sales_curr) * 100.0, 2)
                if profit_curr is not None and sales_curr not in (None, 0)
                else 0.0
            )

        results.append(growth_row)

    return results


#-----------------------------------------------------------------------------
# AI SUMMARY (overall header) â€“ now via ChatGPT with numeric fallback
#-----------------------------------------------------------------------------
summary_numeric_fields = [
    "quantity",
    "net_sales",
    "profit",
    "asp",
    "unit_wise_profitability"
]


def aggregate_totals(rows, fields=None):
    """
    Sum key numeric fields across a list of SKU dicts.
    Used to get total units / sales / profit for each period.
    """
    if fields is None:
        fields = summary_numeric_fields

    totals = {f: 0.0 for f in fields}
    for row in rows:
        for f in fields:
            v = safe_float_local(row.get(f))
            if v is not None:
                totals[f] += v
    return totals


def pct_change(prev, curr):
    if prev is None or prev == 0:
        return None
    return round(((curr - prev) / prev) * 100.0, 1)


def describe_movement(pct):
    """
    Turn a % into a human phrase like 'up 4.2%' / 'strongly down 18.3%' / 'roughly flat'
    """
    if pct is None:
        return "roughly flat"

    abs_v = abs(pct)
    if abs_v < 1:
        return "roughly flat"

    direction = "up" if pct > 0 else "down"
    if abs_v < 5:
        intensity = ""
    elif abs_v < 15:
        intensity = "moderately "
    else:
        intensity = "strongly "

    return f"{intensity}{direction} {abs_v:.1f}%"


def _build_rule_based_summary(prev_totals, curr_totals, top_80_skus, new_reviving,
                              prev_label, curr_label):
    """
    Old numeric summary logic (fallback when AI fails).
    """
    qty_change = pct_change(prev_totals.get("quantity"),   curr_totals.get("quantity"))
    sales_change = pct_change(prev_totals.get("net_sales"), curr_totals.get("net_sales"))
    profit_change = pct_change(prev_totals.get("profit"),   curr_totals.get("profit"))

    bullets = []

    # 1) Overall movement
    bullets.append(
        f"{curr_label} vs {prev_label}: units are {describe_movement(qty_change)}, "
        f"sales are {describe_movement(sales_change)}, and CM1 profit is {describe_movement(profit_change)}."
    )

    # 2) Concentration in top SKUs
    bullets.append(
        f"{len(top_80_skus)} SKUs account for roughly 80% of current sales."
    )

    # 3) New / reviving SKUs
    if new_reviving:
        bullets.append(
            f"{len(new_reviving)} new or reviving SKUs are contributing incremental volume."
        )
    else:
        bullets.append(
            "No material contribution from new or reviving SKUs this period."
        )

    # keep between 2 and 4 bullet points
    return bullets[:4]

def build_sku_context(sku_rows, max_items=5):
    """
    Poore portfolio (ya jo bhi SKU list tum pass karo) ko kuch logical buckets
    me todta hai taaki AI summary/action bullets specific products ka naam
    le sake.

    Ye function AI ko product-focused labels deta hai â€” sirf product_name.
    """

    # ------------------------
    # IMPORTANT: Only product name as label
    # ------------------------
    def make_label(row):
        name = (row.get("product_name") or "").strip()
        sku = (row.get("sku") or "").strip()

        if name:
            return name   # âœ… product name first
        # fallback if product_name missing
        return "Unnamed SKU"

    fast_growing_profitable = []
    declining_high_mix = []
    flat_but_large = []

    def get_pct(row, key):
        obj = row.get(key) or {}
        return safe_float_local(obj.get("value"))

    # ------------------------
    # Build list of items with metrics
    # ------------------------
    for row in sku_rows:
        mix = safe_float_local(row.get("Sales Mix (Current)"))
        if mix is None:
            continue

        unit_g = get_pct(row, "Unit Growth (%)")
        asp_g = get_pct(row, "ASP Growth (%)")
        sales_g = get_pct(row, "Sales Growth (%)")
        mix_g = get_pct(row, "Sales Mix Change (%)")
        up_g   = get_pct(row, "Profit Per Unit (%)")
        prof_g = get_pct(row, "CM1 Profit Impact (%)")

        item = {
            "label": make_label(row),            # âœ… AI ab product name se refer karega
            "sku": row.get("sku"),
            "product_name": row.get("product_name"),

            # current mix
            "sales_mix_curr": mix,

            # % growth
            "unit_growth_pct": unit_g,
            "asp_growth_pct": asp_g,
            "sales_growth_pct": sales_g,
            "mix_change_pct": mix_g,
            "unit_profit_pct": up_g,
            "profit_growth_pct": prof_g,

            # raw previous/current values
            "quantity_prev": safe_float_local(row.get("quantity_prev")),
            "quantity_curr": safe_float_local(row.get("quantity_curr")),
            "asp_prev": safe_float_local(row.get("asp_prev")),
            "asp_curr": safe_float_local(row.get("asp_curr")),
            "net_sales_prev": safe_float_local(row.get("net_sales_prev")),
            "net_sales_curr": safe_float_local(row.get("net_sales_curr")),
            "sales_mix_prev": safe_float_local(row.get("sales_mix_prev")),
            "sales_mix_curr_raw": safe_float_local(row.get("sales_mix_curr")),
            "unit_profit_prev": safe_float_local(row.get("unit_wise_profitability_prev")),
            "unit_profit_curr": safe_float_local(row.get("unit_wise_profitability_curr")),
            "profit_prev": safe_float_local(row.get("profit_prev")),
            "profit_curr": safe_float_local(row.get("profit_curr")),
        }

        # ------------------------
        # Categorization
        # ------------------------
        if (
            sales_g is not None
            and prof_g is not None
            and sales_g > 5
            and prof_g > 5
            and mix >= 2
        ):
            fast_growing_profitable.append(item)

        elif sales_g is not None and sales_g < -5 and mix >= 2:
            declining_high_mix.append(item)

        elif mix >= 5 and (sales_g is None or abs(sales_g) <= 2):
            flat_but_large.append(item)

    # ------------------------
    # Sort & trim
    # ------------------------
    fast_growing_profitable = sorted(
        fast_growing_profitable,
        key=lambda x: x["sales_growth_pct"] or 0,
        reverse=True,
    )[:max_items]

    declining_high_mix = sorted(
        declining_high_mix,
        key=lambda x: x["sales_growth_pct"] or 0,
    )[:max_items]

    flat_but_large = sorted(
        flat_but_large,
        key=lambda x: x["sales_mix_curr"] or 0,
        reverse=True,
    )[:max_items]

    return {
        "fast_growing_profitable": fast_growing_profitable,
        "declining_high_mix": declining_high_mix,
        "flat_but_large": flat_but_large,
    }


def build_ai_summary(
    prev_totals,
    curr_totals,
    top_80_skus,
    new_reviving,
    prev_label,
    curr_label,
    sku_context=None,
):
    def safe0(x):
        v = safe_float_local(x)
        return v if v is not None else 0.0

    # Overall numeric values
    qty_prev = safe0(prev_totals.get("quantity"))
    qty_curr = safe0(curr_totals.get("quantity"))

    sales_prev = safe0(prev_totals.get("net_sales"))
    sales_curr = safe0(curr_totals.get("net_sales"))

    prof_prev = safe0(prev_totals.get("profit"))
    prof_curr = safe0(curr_totals.get("profit"))

    asp_prev_idx = safe0(prev_totals.get("asp"))
    asp_curr_idx = safe0(curr_totals.get("asp"))

    up_prev_idx = safe0(prev_totals.get("unit_wise_profitability"))
    up_curr_idx = safe0(curr_totals.get("unit_wise_profitability"))

    qty_pct = pct_change(qty_prev, qty_curr)
    sales_pct = pct_change(sales_prev, sales_curr)
    prof_pct = pct_change(prof_prev, prof_curr)
    asp_pct = pct_change(asp_prev_idx, asp_curr_idx)
    up_pct = pct_change(up_prev_idx, up_curr_idx)

    if sku_context is None:
        sku_context = {
            "fast_growing_profitable": [],
            "declining_high_mix": [],
            "flat_but_large": [],
        }

    # âœ… IMPORTANT: include SKU rows so product_name is available to the model
    payload = {
        "periods": {
            "previous": {
                "label": prev_label,
                "quantity_total": qty_prev,
                "net_sales_total": sales_prev,
                "profit_total": prof_prev,
                "asp_sum_index": asp_prev_idx,
                "unit_profit_sum_index": up_prev_idx,
            },
            "current": {
                "label": curr_label,
                "quantity_total": qty_curr,
                "net_sales_total": sales_curr,
                "profit_total": prof_curr,
                "asp_sum_index": asp_curr_idx,
                "unit_profit_sum_index": up_curr_idx,
            },
        },
        "pct_changes": {
            "quantity_pct": qty_pct,
            "net_sales_pct": sales_pct,
            "profit_pct": prof_pct,
            "asp_index_pct": asp_pct,
            "unit_profit_index_pct": up_pct,
        },
        "portfolio": {
            "top_80_skus_count": len(top_80_skus),
            "new_reviving_skus_count": len(new_reviving),
        },
        "sku_context": sku_context,
        "sku_tables": {
            "top_80_skus": top_80_skus,
            "new_reviving_skus": new_reviving,
        },
    }

    data_block = json.dumps(payload, indent=2)

    prompt = f"""
You are a senior ecommerce business analyst.

You receive JSON containing:
- Overall totals and % change for units, net sales, profit, ASP index and unit profit index
- SKU tables in sku_tables.top_80_skus and sku_tables.new_reviving_skus including product_name and SKU-wise metrics

GOAL
Produce:
1) A short overall business summary (3â€“5 bullets)
2) Exactly 5 detailed SKU-wise recommendations in the EXACT format below.

====================
SUMMARY (3â€“5 bullets)
====================
Write 3â€“5 short bullets describing, in simple language:
- How overall units, sales and profit moved (using quantity_pct, net_sales_pct, profit_pct)
- Any big change in ASP or unit profit (asp_index_pct, unit_profit_index_pct)
- Whether performance is coming more from volume, pricing, or a few big SKUs

====================
ACTIONS (exactly 5)
====================

Pick SKUs only from:
- sku_tables.top_80_skus
- sku_tables.new_reviving_skus

Return EXACTLY 5 action bullets.

âœ… EACH action bullet MUST follow this exact layout with line breaks:

Line 1: "Product name - <product_name>"
Line 2-3: One paragraph of exactly 2 sentences (metrics + causal chain + mix)
Line 4: (blank line)
Line 5: One action sentence ONLY

So the action_bullet string must look like:
Product name - Classic

The increase in ASP by 13.27% resulted in a dip in units by 25.91%, which also resulted in sales falling by 16.08%. The sales mix is down by 15.93%, reducing its contribution, and profit is up by 10.74%.

Reduce ASP slightly to improve traction.

-----------------------------
Metrics paragraph rules (Line 2-3)
-----------------------------
- Exactly 2 sentences.

- Sentence 1: Use ONLY the SKU metrics for ASP, Units and Sales (where values exist).
  Use the same flow as the original causal chain, but adjust tone only in this special case:
  - Default tone:
    "The increase/decrease in ASP by X% resulted in a dip/growth in units by Y%, which also resulted in sales falling/increasing by Z%."
  - If ASP, Units, and Sales are ALL negative (all down), do NOT imply ASP caused units. Use this co-movement tone instead:
    "There is a decrease in ASP by X% and also a dip in units by Y%, which resulted in sales falling by Z%."

- Sentence 2: Must mention Sales Mix Change (%) if available (up or down) in the same original style:
  "The sales mix is up/down by M%, increasing/reducing its contribution."
  If profitability metrics are available for that SKU, append ONLY ONE of them to the SAME sentence without breaking flow and without using "while/since/because":
  Prefer in this order:
  1) Profit (%)
  2) Profit Per Unit (%)
  Append as:
  ", and profit is up/down by A%."
  OR ", and profit per unit is up/down by B%."
  (If profitability metric is not available, skip it.)

- Do NOT invent numbers and do NOT add extra reasons.

-----------------------------
Allowed action sentences (Line 5 only)
-----------------------------
Use exactly ONE of these sentences, verbatim:
- "Check ads and visibility campaigns for this product."
- "Review the visibility setup for this product."
- "Reduce ASP slightly to improve traction."
- "Increase ASP slightly to strengthen margins."
- "Maintain current ASP and monitor performance."
- "Monitor performance closely for now."
- "Check Amazon fees or taxes for this product as profit is down despite growth."

Decision guidance:
- If ASP is strongly up and units are down: prefer "Reduce ASP slightly to improve traction."
- If units and sales are down and ASP is flat or slightly up: prefer visibility lines.
- If profit/unit profit is very strong and units are stable/slightly down: prefer maintain/increase ASP.
- If ASP, units, sales, and sales mix are up but profit is down: prefer "Check Amazon fees or taxes for this product as profit is down despite growth."

Ignore:
- Any row where product_name is "Total" or contains "Total".


OUTPUT FORMAT
Return ONLY valid JSON:
{{
  "summary_bullets": ["...", "..."],
  "action_bullets": ["Action 1", "Action 2", "Action 3", "Action 4", "Action 5"]
}}
Do not add any extra keys. Do not wrap in Markdown.

DATA
{data_block}
"""

    try:
        ai_response = oa_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a senior ecommerce analyst. "
                        "Return only valid JSON with summary_bullets and action_bullets. "
                        "Each action_bullet must follow the exact 3-block layout: "
                        "Product name line, blank line, 2-sentence metrics paragraph, blank line, one action line."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=900,
            temperature=0.2,
            response_format={"type": "json_object"},
        )

        content = ai_response.choices[0].message.content.strip()
        parsed = json.loads(content)

        sum_bullets = parsed.get("summary_bullets", []) or []
        act_bullets = parsed.get("action_bullets", []) or []

        sum_clean = [str(b).strip() for b in sum_bullets if str(b).strip()]
        act_clean = [str(b).strip() for b in act_bullets if str(b).strip()]

        return {
            "summary_bullets": sum_clean[:5],
            "action_bullets": act_clean[:5],
        }

    except Exception as e:
        print("[ERROR] AI summary generation failed, falling back:", e)

    fallback_summary = _build_rule_based_summary(
        prev_totals,
        curr_totals,
        top_80_skus,
        new_reviving,
        prev_label,
        curr_label,
    )

    fallback_actions = [
        "Product name - Key SKUs\n\nUnits and sales have softened this period. The sales mix has also weakened, reducing contribution.\n\nReview the visibility setup for this product.",
        "Product name - Key SKUs\n\nPricing changes appear to be impacting volume and sales movement. The sales mix change indicates shifting contribution.\n\nMaintain current ASP and monitor performance.",
        "Product name - Key SKUs\n\nSome high-mix SKUs show volume decline impacting sales outcomes. The sales mix has moved down, reducing contribution.\n\nCheck ads and visibility campaigns for this product.",
        "Product name - Key SKUs\n\nProfitability is improving even where sales are softer across some SKUs. The sales mix change shows contribution movement.\n\nMonitor performance closely for now.",
        "Product name - Key SKUs\n\nA few SKUs may need pricing support to improve traction. The sales mix indicates contribution shifts.\n\nReduce ASP slightly to improve traction.",
    ]

    return {
        "summary_bullets": fallback_summary,
        "action_bullets": fallback_actions[:5],
    }


#-----------------------------------------------------------------------------
# ChatGPT insight generator for live MTD vs previous (per-SKU)
#-----------------------------------------------------------------------------
def generate_live_insight(item, country, prev_label, curr_label):
    """
    Generate AI insight for a single SKU row from live_mtd_vs_previous growth_data.
    item: one dict from growth_data/top_80_skus/new_reviving/etc.
    """
    sku = item.get("sku")
    product_name = (item.get("product_name") or "this product").strip()

    # âœ… single, deterministic key (no global logic)
    key = sku or product_name

    is_new_or_reviving = item.get("new_or_reviving", False)

    # Data for the model (includes fields like quantity_prev/curr, net_sales_prev/curr, growth %, etc.)
    data_block = json.dumps(item, indent=2)

    if is_new_or_reviving:
        # New / reviving SKU prompt
        prompt = f"""
    You are a senior ecommerce business analyst. The following is a new or reviving product (no meaningful previous-period baseline).
    Compare it only within the current period and talk about its launch strength.

    Context:
    - Country: {country}
    - Previous period label: {prev_label}
    - Current period label: {curr_label}

    Details for '{product_name}'

    Observations:
    - List the 2â€“3 most important observations about this product's current performance
    (units sold, ASP, net sales, profit per unit, etc.) using absolute values from the data.
    - Comment on launch/return momentumâ€”e.g., strong debut, moderate start, slow start.
    - Call out any potential red flags (e.g., high ASP but very low volume, low unit profitability, etc.).

    Improvements:
    - Suggest clear, concrete next actions for:
    â€¢ Marketing
    â€¢ Sales / Commercial
    â€¢ Operations / Supply
    - Make each action specific and easy to execute.

    Sales Volume:
    â€¢ Comment on volume and what it says about early traction.
    â€¢ Suggest one commercial lever to improve or scale volume.

    ASP:
    â€¢ Comment on price positioning; suggest whether to test price up/down or hold.

    Profitability:
    â€¢ Comment on profit per unit or total profit; suggest if costs or pricing need optimization.

    End with one line:
    â€¢ Verdict: should this SKU be scaled quickly, tested more, or carefully repositioned? And why?

    Instructions:
    - Use plain text with bullet points only.
    - DO NOT use Markdown formatting (no **bold**, no headings).
    - Do NOT compare to previous periods (assume no baseline).
    - Use actual numbers or percentages from the data whenever they are present.

    Data:
    {data_block}
    """
    else:
        # Existing SKU with prev vs current
        prompt = f"""
    You are a senior ecommerce business analyst. The data below shows a product's performance
    comparing a previous period vs the current MTD.

    Context:
    - Country: {country}
    - Previous period: {prev_label}
    - Current period: {curr_label}

    Details for '{product_name}'

    Observations:
    - List the 2â€“3 most important changes using ONLY the given metrics:
    â€¢ quantity_prev vs quantity_curr
    â€¢ net_sales_prev vs net_sales_curr
    â€¢ profit_prev vs profit_curr
    â€¢ asp_prev vs asp_curr
    â€¢ unit_wise_profitability_prev vs unit_wise_profitability_curr
    â€¢ and % fields like "Unit Growth (%)", "Sales Growth (%)", etc.
    - Use the exact causal tone wherever % values exist:
    "The increase/decrease in ASP by X% resulted in a dip/growth in units by Y%, which also resulted in sales falling/increasing by Z%."
    - In at least one observation, mention Sales Mix Change (%) direction if present (up/down).
    - Do NOT add assumptions like stock issues, supply constraints, replenishment, OOS, or fulfillment problems.

    Improvements:
    - Provide exactly 3â€“5 action bullets.
    - Each action bullet MUST be exactly ONE sentence and MUST be chosen ONLY from the list below, verbatim (no edits):
    â€¢ "Check ads and visibility campaigns for this product."
    â€¢ "Review the visibility setup for this product."
    â€¢ "Reduce ASP slightly to improve traction."
    â€¢ "Increase ASP slightly to strengthen margins."
    â€¢ "Monitor performance closely and reassess next steps."
    â€¢ "Monitor performance closely for now."
    - Do NOT add any other recommendations, explanations, or extra words.
    - Do NOT mention stock, inventory, supply, operations, OOS, logistics, replenishment, or warehousing.
    - Decision guidance:
    â€¢ If ASP is strongly up and units are down: prefer "Reduce ASP slightly to improve traction."
    â€¢ If units and sales are down and ASP is flat or slightly up: prefer visibility lines.
    â€¢ If profit/unit profit is very strong and units are stable/slightly down: prefer maintain/increase ASP.

    Then, for each metric, add:

    Unit Growth:
    â€¢ [Explain reasons for the growth/decline using ONLY available signals like unit trend vs ASP trend and what that implies about demand/visibility/conversion.]
    â€¢ [Choose ONE action bullet from the Improvements list that best fits the unit pattern and paste it verbatim.]

    ASP:
    â€¢ [Explain why ASP changed using ONLY available signals like pricing changes, discounting intensity, or product/pack/channel mix shifts (premium vs value) without referencing costs.]
    â€¢ [Choose ONE action bullet from the Improvements list that best fits the ASP direction and paste it verbatim.]

    Sales:
    â€¢ [Describe sales trend by explicitly tying it to Units Ã— ASP (e.g., â€œsales down mainly due to units decline while ASP was flat/upâ€ or â€œsales up driven by ASP lift with stable unitsâ€).]
    â€¢ [Choose ONE action bullet from the Improvements list that best fits the sales pattern and paste it verbatim.]

    Profit:
    â€¢ [Explain profit change using ONLY available signals like sales movement plus realized pricing/discounting/mix impact, and avoid any mention of COGS or cost changes.]
    â€¢ [Choose ONE action bullet from the Improvements list that best aligns with protecting/improving profitability given the observed trend and paste it verbatim.]

    Unit Profitability:
    â€¢ [Explain per-unit profit change using ONLY available signals like realized price/discounting and mix (higher-priced variants) impact, without mentioning COGS.]
    â€¢ [Choose ONE action bullet from the Improvements list that best fits per-unit profit strength/weakness and paste it verbatim.]

    Instructions:
    - Use plain text with bullets only.
    - DO NOT use Markdown formatting (no **bold**, no italics, no headers).
    - Avoid labels like "Root cause:" or "Action item:". Just use bullet points.
    - Use % values and trends from the data for every observation.
    - Make all insights easy for business teams to act on.

    Data:
    {data_block}
    """

    try:
        ai_response = oa_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a senior ecommerce analyst. "
                        "Respond in plain text with bullet points only. "
                        "Do not use Markdown formatting."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=900,
            temperature=0.4,
        )

        ai_text = ai_response.choices[0].message.content.strip()

        # ===== DEBUG: print exactly what model returned =====
        print("\n================ AI INSIGHT DEBUG ================")
        print("KEY:", key, "| SKU:", sku, "| Product:", product_name, "| New/Reviving:", is_new_or_reviving)
        print("INSIGHT (raw):\n", ai_text)
        print("INSIGHT (repr):\n", repr(ai_text))
        print("==================================================\n")

        return key, {
            "sku": sku,
            "product_name": product_name,
            "insight": ai_text,
            "key_used": key,
            "is_new_or_reviving": is_new_or_reviving,
        }

    except Exception as e:
        return key, {
            "sku": sku,
            "product_name": product_name,
            "insight": f"Error generating insight: {str(e)}",
            "key_used": key,
            "is_new_or_reviving": is_new_or_reviving,
        }

# -----------------------------------------------------------------------------
# MAIN ROUTE: LIVE MTD vs PREVIOUS-MONTH-SAME-PERIOD BI
# # -----------------------------------------------------------------------------

@live_data_bi_bp.route("/live_mtd_bi", methods=["GET"])
def live_mtd_vs_previous():
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        return jsonify({"error": "Authorization token is missing or invalid"}), 401

    token = auth_header.split(" ")[1]

    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("user_id")
        if not user_id:
            return jsonify({"error": "Invalid token payload: user_id missing"}), 401

        country = request.args.get("countryName", "uk")
        as_of = request.args.get("as_of")

        # optional custom day range
        start_day_str = request.args.get("start_day")
        end_day_str = request.args.get("end_day")
        try:
            start_day = int(start_day_str) if start_day_str else None
            end_day = int(end_day_str) if end_day_str else None
        except ValueError:
            start_day = None
            end_day = None

        generate_ai_insights = (
            request.args.get("generate_ai_insights", "false").lower()
            in ("true", "1", "yes")
        )

        ranges = get_mtd_and_prev_ranges(
            as_of=as_of,
            start_day=start_day,
            end_day=end_day,
        )
        prev_start = ranges["previous"]["start"]
        prev_end = ranges["previous"]["end"]
        curr_start = ranges["current"]["start"]
        curr_end = ranges["current"]["end"]

        # âœ… FULL previous month ONLY for graph
        prev_full_start = date(
            ranges["meta"]["previous_year"],
            ranges["meta"]["previous_month"],
            1
        )
        last_day_prev = monthrange(prev_full_start.year, prev_full_start.month)[1]
        prev_full_end = date(prev_full_start.year, prev_full_start.month, last_day_prev)

        # is_global = country.lower() == "global"
        key_column = "sku"

        # 1) fetch previous period (ALIGNED) -> table/growth/totals/AI
        prev_data_aligned, prev_daily_aligned = fetch_previous_period_data(
            user_id, country, prev_start, prev_end
        )

        # 1b) fetch previous period (FULL MONTH) -> graph only
        _, prev_daily_full = fetch_previous_period_data(
            user_id, country, prev_full_start, prev_full_end
        )

        # 2) fetch current MTD (âœ… now includes __has_mapping__ in each row)
        curr_data, curr_daily = fetch_current_mtd_data(
        user_id, country, curr_start, curr_end
        )


        # 3) growth calculation (use ALIGNED prev)
        growth_data = calculate_growth(prev_data_aligned, curr_data, key=key_column)

        # 4) categorization (use ALIGNED prev)
        prev_keys = {row.get(key_column) for row in prev_data_aligned if row.get(key_column)}

    
        # ==========================================================
        # âœ… STEP 5: New / Reviving SKUs (Historic BI parity: 6-month check)
        # ==========================================================
        # Logic:
        # - Reviving: SKU present in current, absent in previous aligned period
        # - New: SKU not seen in last 6 months (excluding current month)
        # - Final: union of (reviving âˆª new)

        curr_keys = {row.get(key_column) for row in curr_data if row.get(key_column)}

        historical_6m_keys = fetch_historical_skus_last_6_months(
            user_id=user_id,
            country=country,
            ref_date=curr_start
        )

        reviving_keys = curr_keys - prev_keys
        newly_launched_keys = curr_keys - historical_6m_keys
        new_reviving_keys = reviving_keys | newly_launched_keys

        new_reviving = [
            row for row in growth_data
            if row.get(key_column) in new_reviving_keys
        ]


        # ==========================================================
        # âœ… STEP 6: Exclude new_reviving (incl. unmapped) from Top80/Other pipeline
        # ==========================================================

        new_reviving_keys = {r.get(key_column) for r in new_reviving if r.get(key_column)}

        existing = [
            row
            for row in growth_data
            if row.get(key_column) in prev_keys
            and row.get("Sales Mix (Current)") is not None
            and row.get(key_column) not in new_reviving_keys   # âœ… exclude
        ]

        existing_sorted = sorted(
            existing, key=lambda x: x["Sales Mix (Current)"], reverse=True
        )

        total_sales_mix = sum(
            row["Sales Mix (Current)"]
            for row in existing_sorted
            if row["Sales Mix (Current)"] is not None
        )

        cumulative = 0.0
        top_80_skus, other_skus = [], []

        for row in existing_sorted:
            mix = row["Sales Mix (Current)"]
            if mix is None:
                continue
            proportion = cumulative / total_sales_mix if total_sales_mix else 0
            if proportion <= 0.8:
                top_80_skus.append(row)
                cumulative += mix
            else:
                other_skus.append(row)

        # ------- segment subsets (prev / curr data) -------
        top_keys = {row.get(key_column) for row in top_80_skus}
        other_keys = {row.get(key_column) for row in other_skus}
        new_keys = {row.get(key_column) for row in new_reviving if row.get(key_column)}

        prev_top = [r for r in prev_data_aligned if r.get(key_column) in top_keys]
        curr_top = [r for r in curr_data if r.get(key_column) in top_keys]

        prev_other = [r for r in prev_data_aligned if r.get(key_column) in other_keys]
        curr_other = [r for r in curr_data if r.get(key_column) in other_keys]

        prev_new = [r for r in prev_data_aligned if r.get(key_column) in new_keys]
        curr_new = [r for r in curr_data if r.get(key_column) in new_keys]

        # ------- segment total rows -------
        top_80_total_row = build_segment_total_row(
            prev_top, curr_top, key=key_column, label="Total"
        )
        new_reviving_total_row = (
            build_segment_total_row(
                prev_new, curr_new, key=key_column, label="Total"
            )
            if new_reviving
            else None
        )
        other_total_row = (
            build_segment_total_row(
                prev_other, curr_other, key=key_column, label="Total"
            )
            if other_skus
            else None
        )

        # labels
        prev_label = (
            f"{month_abbr[prev_start.month].capitalize()}"
            f"'{str(prev_start.year)[-2:]} {prev_start.day}â€“{prev_end.day}"
        )
        curr_label = (
            f"{month_abbr[curr_start.month].capitalize()}"
            f"'{str(curr_start.year)[-2:]} {curr_start.day}â€“{curr_end.day}"
        )
        prev_label_full = (
            f"{month_abbr[prev_full_start.month].capitalize()}"
            f"'{str(prev_full_start.year)[-2:]} 1â€“{prev_full_end.day}"
        )

        # ============================
        # OVERALL SUMMARY + ACTIONS
        # ============================
        prev_totals = aggregate_totals(prev_data_aligned)
        curr_totals = aggregate_totals(curr_data)

        all_for_context = growth_data
        sku_context = build_sku_context(all_for_context, max_items=5)

        overall = build_ai_summary(
            prev_totals,
            curr_totals,
            top_80_skus,
            new_reviving,
            prev_label,
            curr_label,
            sku_context=sku_context,
        )

        overall_summary = overall.get("summary_bullets", [])
        overall_actions = overall.get("action_bullets", [])

        # ============================
        # SEND EMAIL WITH AI SUMMARY
        # ============================

        # 1) Try to read email from JWT payload, else from query param
        user_email = payload.get("email") or request.args.get("email")
        if not user_email:
            user_email = get_user_email_by_id(user_id)


        if user_email:
            # 3) Throttle: only once in 24 hours per user+country
            if has_recent_bi_email(user_id, country, hours=24):
                print(f"[INFO] BI email already sent in last 24h for user_id={user_id}, country={country}; skipping.")
            else:
                # 4) Generate a fresh token for this user for deep-link
                email_token_payload = {
                    "user_id": user_id,
                    "email": user_email,
                    "scope": "live_mtd_bi",
                    "exp": datetime.utcnow() + timedelta(hours=24),
                }
                email_token = jwt.encode(email_token_payload, SECRET_KEY, algorithm="HS256")

                try:
                    send_live_bi_email(
                        to_email=user_email,
                        overall_summary=overall_summary,
                        overall_actions=overall_actions,  # âœ… now valid
                        sku_actions=None,                 # or pass structured list when you have it
                        country=country,
                        prev_label=prev_label,
                        curr_label=curr_label,
                        deep_link_token=email_token,
                    )

                    mark_bi_email_sent(user_id, country)
                except Exception as e:
                    # Don't break the API if email fails
                    print(f"[WARN] Error sending live BI email: {e}")
        else:
            print("[WARN] No user email found in token, query params, or DB; skipping BI email.")



        # ============================
        # AI INSIGHTS (SKU-wise)
        # ============================
        skus_for_ai = top_80_skus + new_reviving + other_skus
        insights = {}

        if generate_ai_insights and skus_for_ai:
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_item = {
                    executor.submit(
                        generate_live_insight,
                        item,
                        country,
                        prev_label,
                        curr_label,
                    ): item
                    for item in skus_for_ai
                }

                for future in as_completed(future_to_item):
                    try:
                        key, result = future.result()
                        insights[key] = result
                    except Exception as e:
                        item = future_to_item[future]
                        print(
                            f"Error generating AI insight for SKU={item.get('sku')} "
                            f"Product={item.get('product_name')}: {e}"
                        )

        # ============================
        # RESPONSE
        # ============================
        response_payload = {
            "message": "Live MTD vs previous-month-same-period comparison",
            "periods": {
                "previous": {
                    "label": prev_label,
                    "start_date": prev_start.isoformat(),
                    "end_date": prev_end.isoformat(),
                },
                "previous_full": {
                    "label": prev_label_full,
                    "start_date": prev_full_start.isoformat(),
                    "end_date": prev_full_end.isoformat(),
                },
                "current_mtd": {
                    "label": curr_label,
                    "start_date": curr_start.isoformat(),
                    "end_date": curr_end.isoformat(),
                },
            },
            "categorized_growth": {
                "top_80_skus": top_80_skus,
                "top_80_total": top_80_total_row,
                "new_or_reviving_skus": new_reviving,  # âœ… includes unmapped now
                "new_or_reviving_total": new_reviving_total_row,
                "other_skus": other_skus,
                "other_total": other_total_row,
            },
            "daily_series": {
                "previous": prev_daily_full,
                "current_mtd": curr_daily,
            },
            "daily_series_aligned": {
                "previous": prev_daily_aligned,
                "current_mtd": curr_daily,
            },
            "ai_insights": insights,
            "overall_summary": overall_summary,
            "overall_actions": overall_actions,
        }

        response_payload = round_numeric_values(response_payload, ndigits=2)
        return jsonify(response_payload), 200

    except jwt.ExpiredSignatureError:
        return jsonify({"error": "Token has expired"}), 401
    except jwt.InvalidTokenError:
        return jsonify({"error": "Invalid token"}), 401
    except Exception as e:
        print("Unexpected error in /live_mtd_bi:", e)
        return jsonify({"error": "Server error", "details": str(e)}), 500

